---
title: "Diss Stats"
output: html_document
date: "2024-03-05"
---
#library import
```{r}
#Import the libraries and the Dataset

library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(scales)
library(MASS)

# Set the working directory to the folder where your data is supposed to be
setwd("/Users/diegolucini/OneDrive - University of Edinburgh/Year 4/Diss/Raw Data")

data <- read.csv("Official R Data Dissertation.csv")
head(data)
```
```{r}
getwd()

```

#1 Descriptive Statistics

#year
```{r}
# Calculate the total number of papers for each year
yearly_totals <- data %>% 
  group_by(Year) %>% 
  summarise(Total = n(), .groups = "drop")

grand_total <- sum(yearly_totals$Total)

yearly_totals <- yearly_totals %>% 
  mutate(Percentage = (Total / grand_total) * 100)

print(yearly_totals)

#frequency and percentage of paper in each year
total_counts_by_type <- data %>%
  group_by(Institution) %>%
  summarise(Total = n(), .groups = "drop")
print(total_counts_by_type)

summarized_data <- data %>%
  group_by(Year, Institution) %>%
  summarise(count = n(), .groups = "drop") %>%
  left_join(total_counts_by_type, by = "Institution") %>%
  mutate(Percentage = (count / Total) * 100) %>%
  

print(summarized_data)

```
#DAS
```{r}
# Calculate the total number of papers for each year
yearly_totals <- data %>% 
  group_by(DAS) %>% 
  summarise(Total = n(), .groups = "drop")

grand_total <- sum(yearly_totals$Total)

yearly_totals <- yearly_totals %>% 
  mutate(Percentage = (Total / grand_total) * 100)

print(yearly_totals)

#frequency and percentage of paper in each year
total_counts_by_type <- data %>%
  group_by(Institution) %>%
  summarise(Total = n(), .groups = "drop")
print(total_counts_by_type)

summarized_data <- data %>%
  group_by(DAS, Institution) %>%
  summarise(count = n(), .groups = "drop") %>%
  left_join(total_counts_by_type, by = "Institution") %>%
  mutate(Percentage = (count / Total) * 100) %>%
  

print(summarized_data)

```


#FAIR implementation
```{r}
data <- data %>%
  mutate(Period = ifelse(Year <= 2016, "Before or in 2016", "After 2016"))

# Calculate the total number of papers for each period
period_totals <- data %>%
  group_by(Period) %>%
  summarise(Total = n(), .groups = "drop")

# Calculate the grand total of all papers
grand_total <- sum(period_totals$Total)

# Add a column for the percentage of each period's total relative to the grand total
period_totals <- period_totals %>%
  mutate(Percentage = (Total / grand_total) * 100)

# Print the period totals and percentages
print(period_totals)

#to categorize papers based on the year of publication compared to FAIR principles implementation 2016 (depend on institute)

#Frequency and Percantage of paper before and After FAIR principles (2016)
total_counts_by_type <- data %>%
  group_by(Institution) %>%
  summarise(Total = n(), .groups = "drop")

# Categorize, calculate frequency and percentage for each Type based on year
summarized_data <- data %>%
  mutate(Category = ifelse(Year <= 2016, "On/Before 2016", "After 2020")) %>%
  group_by(Category, Institution) %>%
  summarise(count = n(), .groups = "drop") %>%
  left_join(total_counts_by_type, by = "Institution") %>%
  mutate(Percentage = (count / Total) * 100) %>%

print(summarized_data)
```


#COVID (Before and After)
```{r}
data <- data %>%
  mutate(Period = ifelse(Year <= 2020, "Before or in 2020", "After 2020"))

# Calculate the total number of papers for each period
period_totals <- data %>%
  group_by(Period) %>%
  summarise(Total = n(), .groups = "drop")

# Calculate the grand total of all papers
grand_total <- sum(period_totals$Total)

# Add a column for the percentage of each period's total relative to the grand total
period_totals <- period_totals %>%
  mutate(Percentage = (Total / grand_total) * 100)

# Print the period totals and percentages
print(period_totals)

total_counts_by_type <- data %>%
  group_by(Institution) %>%
  summarise(Total = n(), .groups = "drop")

# Categorize, calculate frequency and percentage for each Type based on year
summarized_data <- data %>%
  mutate(Category = ifelse(Year <= 2020, "On/Before 2020", "After 2020")) %>%
  group_by(Category, Institution) %>%
  summarise(count = n(), .groups = "drop") %>%
  left_join(total_counts_by_type, by = "Institution") %>%
  mutate(Percentage = (count / Total) * 100) %>%

print(summarized_data)
```
#plot distribution of the included papers over the years
```{r}
library(ggplot2)
aggregated_data <- data %>%
  group_by(Year, Institution) %>%
  summarise(count = n(), .groups = "drop")

# Create the clustered bar chart with numbers on top of the bars and specified colors
year_plot <- ggplot(aggregated_data, aes(x = as.factor(Year), y = count, fill = Institution)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = count), position = position_dodge(width = 0.9), vjust = -0.5) +
  scale_fill_manual(values = c("#FDCF60", "#1A56BB", "#AE4371")) +
  labs(x = "Year", y = "Number of Papers", title = "Number of Papers by Year") +
  theme_minimal()

print(year_plot)
```

# Frequency and Percentage of the papers that share data (Data Sharing= Data Completeness score >2) Overall
```{r}

data <- data %>%
  mutate(Period = ifelse(Complete <= 2, "Completeness <= 2", "Completeness > 2"))

# Calculate the total number of papers for each period
period_totals <- data %>%
  group_by(Period) %>%
  summarise(Total = n(), .groups = "drop")

# Calculate the grand total of all papers
grand_total <- sum(period_totals$Total)

# Add a column for the percentage of each period's total relative to the grand total
period_totals <- period_totals %>%
  mutate(Percentage = (Total / grand_total) * 100)

# Print the period totals and percentages
print(period_totals)
```
# Frequency and Percentage of the papers that share data (Data Sharing= Data Completeness score >1) by Institution
```{r}
total_counts_by_type <- data %>%
  group_by(Institution) %>%
  summarise(Total = n(), .groups = "drop")

# Categorize based on Completeness, calculate frequency and percentage for each Institution
summarized_data <- data %>%
  filter(Complete <= 2  | Complete > 2) %>%
  mutate(Category = ifelse(Complete <= 2, "Completeness <= 2", "Completeness > 2")) %>%
  group_by(Category, Institution) %>%
  summarise(count = n(), .groups = "drop") %>%
  left_join(total_counts_by_type, by = "Institution") %>%
  mutate(Percentage = (count / Total) * 100) %>%

print(summarized_data)
```

#Chi squared test to assess difference in data share between three types of institutions (IIIR, IQB3, Roslin)
```{r}
#create contigency table
table_completeness <- table(data$Institution, data$Complete <= 2)

# Perform the Chi-square test
result <- chisq.test(table_completeness)

# Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)

```
#Preprints
```{r}
# Calculate frequency and percentage for each Preprints category, across all types
preprints_summary <- data %>%
  group_by(Preprints) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(preprints_summary$count)

# Add a column for the percentage of each Preprints category relative to the overall total
preprints_summary <- preprints_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(preprints_summary)

# Frequency and Percentage of the papers with preprints
data %>%
  group_by(Institution, Preprints) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Preprints)
```
#Preprint chi-square test to assess if there is any difference in preprint between the 3 institutes
```{r}
# Create a contingency table
table_preprint <- table(data$Institution, data$Preprints)

# Perform the Chi-square test
result <- chisq.test(table_preprint)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```
#DAS
```{r}
DAS_summary <- data %>%
  group_by(DAS) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(DAS_summary$count)

# Add a column for the percentage of each DAS category relative to the overall total
DAS_summary <- DAS_summary %>%
  mutate(percentage = (count / overall_total) * 100)

print(DAS_summary)

data$DAS <- as.factor(data$DAS)

# Frequency and Percentage of the papers with DAS in the publications
data %>%
  group_by(Institution, DAS) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, DAS)
```
#Chi square test to assess if there is any difference in DAS between the 3 institutes

```{r}
# Create a contingency table
table_das <- table(data$Institution, data$DAS)

# Perform the Chi-square test
result <- chisq.test(table_das)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```
# Calculate frequency and percentage for Code Sharing
```{r}
CodeArchived_summary <- data %>%
  group_by(CodeArchived) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(CodeArchived_summary$count)

# Add a column for the percentage of each DAS category relative to the overall total
CodeArchived_summary <- CodeArchived_summary %>%
  mutate(percentage = (count / overall_total) * 100)

print(CodeArchived_summary)

# Frequency and Percentage of the papers that shared the code used
data %>%
  group_by(Institution, CodeArchived) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%

  arrange(Institution, CodeArchived)
```
#Chi square test to assess if there is any difference in Code Sharing between the institutions

```{r}
# Create a contingency table
table_code <- table(data$Institution, data$CodeArchived)

# Perform the Chi-square test
result <- chisq.test(table_code)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```
#image data
```{r}
image_summary <- data %>%
  group_by(Image) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(image_summary$count)

# Add a column for the percentage of each Image data category relative to the overall total
image_summary <- image_summary %>%
  mutate(percentage = (count / overall_total) * 100)

print(image_summary)

data$Image <- as.factor(data$Image)

# Frequency and Percentage of the papers with image data in the publications
data %>%
  group_by(Institution, Image) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Image)
```
#Chi square test to assess if there is any difference in image data sharing between the institutions
```{r}
# Create a contingency table
table_code <- table(data$Institution, data$Image)

# Perform the Chi-square test
result <- chisq.test(table_code)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```

#genome data
```{r}
genomics_summary <- data %>%
  group_by(Genomics) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(genomics_summary$count)

# Add a column for the percentage of each genome data category relative to the overall total
genomics_summary <- genomics_summary %>%
  mutate(percentage = (count / overall_total) * 100)

print(genomics_summary)

data$Genomics <- as.factor(data$Genomics)

# Frequency and Percentage of the papers with genomics data in the publications
data %>%
  group_by(Institution, Genomics) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Genomics)
```
#proteomics data
```{r}
proteomic_summary <- data %>%
  group_by(Proteomic.Data) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(proteomic_summary$count)

# Add a column for the percentage of each proteomic data category relative to the overall total
proteomic_summary <- proteomic_summary %>%
  mutate(percentage = (count / overall_total) * 100)

print(proteomic_summary)

data$Proteomic.Data <- as.factor(data$Proteomic.Data)

# Frequency and Percentage of the papers with proteomic data in the publications
data %>%
  group_by(Institution, Proteomic.Data) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Proteomic.Data)
```

#Human data
```{r}
human_summary <- data %>%
  group_by(Human.Data) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(human_summary$count)

# Add a column for the percentage of each human data category relative to the overall total
human_summary <- human_summary %>%
  mutate(percentage = (count / overall_total) * 100)

print(human_summary)

data$Human.Data <- as.factor(data$Human.Data)

# Frequency and Percentage of the papers with human data in the publications
data %>%
  group_by(Institution, Human.Data) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Human.Data)
```
#Chi square test to assess if there is any difference in Human data sharing between the institutions
```{r}
# Create a contingency table
table_code <- table(data$Institution, data$Human.Data)

# Perform the Chi-square test
result <- chisq.test(table_code)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```

#Analysis Program
```{r}
# Calculate frequency and percentage for each anlaysis program category, across all types
analysispgrm_summary <- data %>%
  group_by(AnalysisPgrm) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(analysispgrm_summary$count)

# Add a column for the percentage of each analysis program category relative to the overall total
analysispgrm_summary <- analysispgrm_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(analysispgrm_summary)

# Frequency and Percentage of the papers with preprints
data %>%
  group_by(Institution, AnalysisPgrm) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, AnalysisPgrm)
```
#Journal Access
```{r}
# Calculate frequency and percentage for each journal access category, across all types
journalaccess_summary <- data %>%
  group_by(Journal.Access) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(journalaccess_summary$count)

# Add a column for the percentage of each journal access category relative to the overall total
journalaccess_summary <- journalaccess_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(journalaccess_summary)

# Frequency and Percentage of the papers with journal access
data %>%
  group_by(Institution, Journal.Access) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Journal.Access)
```
#Ethics statement
```{r}
# Replace "2" with "1" in the Ethics.Statement column
data_modified <- data %>%
  mutate(Ethics.Statement = ifelse(Ethics.Statement == 2, 1, Ethics.Statement))

# Calculate frequency and percentage for each ethics statement category, across all types
ethics_summary <- data_modified %>%
  group_by(Ethics.Statement) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(ethics_summary$count)

# Add a column for the percentage of each ethics statement category relative to the overall total
ethics_summary <- ethics_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(ethics_summary)

# Frequency and Percentage of the papers with ethics statements
data_modified %>%
  group_by(Institution, Ethics.Statement) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Ethics.Statement)
``` 

#Infectious disease
```{r}
# Calculate frequency and percentage for each infectious disease category, across all types
infectiousdisease_summary <- data %>%
  group_by(Infectious.Disease) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(infectiousdisease_summary$count)

# Add a column for the percentage of each infection category relative to the overall total
infectiousdisease_summary <- infectiousdisease_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(infectiousdisease_summary)

# Frequency and Percentage of the papers with infectious diseases
data %>%
  group_by(Institution, Infectious.Disease) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Infectious.Disease)
```
#Treatment studied
```{r}
# Calculate frequency and percentage for each treatment category, across all types
treatment_summary <- data %>%
  group_by(Treatment.development) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(treatment_summary$count)

# Add a column for the percentage of each treatment category relative to the overall total
treatment_summary <- treatment_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(treatment_summary)

# Frequency and Percentage of the papers with treatment development
data %>%
  group_by(Institution, Treatment.development) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Treatment.development)
```

#Subject engagement
```{r}
print(data$Subject.Engagement)

data$Subject.Engagement <- factor(data$Subject.Engagement)
subjecttable <- table(data$Institution, data$Subject.Engagement)

print(subjecttable)
#Consider 2 as 1
data_modified <- data %>%
  mutate(Subject.Engagement = ifelse(Subject.Engagement == 2, 1, Subject.Engagement))

# Calculate frequency and percentage for each subject category, across all types
subject_summary <- data_modified %>%
  group_by(Subject.Engagement) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(subject_summary$count)

# Add a column for the percentage of each subject category relative to the overall total
subject_summary <- subject_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(subject_summary)

# Frequency and Percentage of the papers with subject engagement
data_modified %>%
  group_by(Institution, Subject.Engagement) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Subject.Engagement)
```

```{r}
# Print the current status of Subject.Engagement
print(data$Subject.Engagement)

# Factor conversion, ensuring NA values are not automatically excluded
data$Subject.Engagement <- factor(data$Subject.Engagement, exclude=NULL)

# Create a contingency table, including NA values
subjecttable <- table(data$Institution, data$Subject.Engagement, useNA = "ifany")

# Print the contingency table
print(subjecttable)


# Calculate frequency and percentage for each subject category, across all types
subject_summary <- data %>%
  group_by(Subject.Engagement) %>%
  summarise(count = n(), .groups = "drop")

# Calculate the overall total, including NA
overall_total <- sum(subject_summary$count, na.rm = TRUE)

# Add a column for the percentage of each subject category relative to the overall total
subject_summary <- subject_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(subject_summary)

# Frequency and Percentage of the papers with subject engagement, including handling for NA
data %>%
  group_by(Institution, Subject.Engagement) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count, na.rm = TRUE), 
         percentage = (count / total) * 100) %>%
  ungroup() %>%
  arrange(Institution, Subject.Engagement)

```

#Participant location
```{r}
# Calculate frequency and percentage for each participant location category, across all types
participant_summary <- data %>%
  group_by(Participant.location) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(participant_summary$count)

# Add a column for the percentage of each partiocipant location category relative to the overall total
participant_summary <- participant_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(participant_summary)

# Frequency and Percentage of the papers with treatment development
data %>%
  group_by(Institution, Participant.location) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Participant.location)
```

#Human intervention study
```{r}
# Calculate frequency and percentage for each intervention study category, across all types
intervention_summary <- data %>%
  group_by(Human.intervention.study) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(intervention_summary$count)

# Add a column for the percentage of each intervention category relative to the overall total
intervention_summary <- intervention_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(intervention_summary)

# Frequency and Percentage of the papers with treatment development
data %>%
  group_by(Institution, Human.intervention.study) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Human.intervention.study)
```
#Global vs Local Disease Distribution
```{r}
# Calculate frequency and percentage for each disease distribution category, across all types
diseasedistribution_summary <- data %>%
  group_by(Global.or.local.) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(diseasedistribution_summary$count)

# Add a column for the percentage of each intervention category relative to the overall total
diseasedistribution_summary <- diseasedistribution_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(diseasedistribution_summary)

# Frequency and Percentage of the papers with treatment development
data %>%
  group_by(Institution, Global.or.local.) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Global.or.local.)
```
#Corresponding Author
```{r}

# Recode the values of CorresAuthor
data <- data %>%
  mutate(CorresAuthor = recode(CorresAuthor, `0` = "0", `1` = "1", `2` = "0", `3` = "0"))

# Calculate frequency and percentage for each corresponding author category, across all types
correspondingauthor_summary <- data %>%
  group_by(CorresAuthor) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(correspondingauthor_summary$count)

# Add a column for the percentage of each corresponding author category relative to the overall total
correspondingauthor_summary <- correspondingauthor_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(correspondingauthor_summary)

# Frequency and Percentage of the papers with treatment development
data %>%
  group_by(Institution, CorresAuthor) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, CorresAuthor)
```
#Conflicts of interest
```{r}
# Calculate frequency and percentage for each conflict of interest category, across all types
conflictsinterest_summary <- data %>%
  group_by(Conflict.of.Interest) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(conflictsinterest_summary$count)

# Add a column for the percentage of each conflict category relative to the overall total
conflictsinterest_summary <- conflictsinterest_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(conflictsinterest_summary)

# Frequency and Percentage of the papers with conflict interest
data %>%
  group_by(Institution, Conflict.of.Interest) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Conflict.of.Interest)
```
#Funding type
```{r}
# Calculate frequency and percentage for each funding category, across all types
funding_summary <- data %>%
  group_by(Funding.Type) %>%
  summarise(count = n(), .groups = "drop") 

# Calculate the overall total
overall_total <- sum(funding_summary$count)

# Add a column for the percentage of each conflict category relative to the overall total
funding_summary <- funding_summary %>%
  mutate(percentage = (count / overall_total) * 100)

# Print the summary
print(funding_summary)

# Frequency and Percentage of the papers with funding type
data %>%
  group_by(Institution, Funding.Type) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(Institution) %>%
  mutate(total = sum(count), 
         percentage = (count/total)*100) %>%
  ungroup() %>%
  arrange(Institution, Funding.Type)
```

#Plot the 4 scoring criteria for all the papers
```{r}
# Convert all relevant columns to factors first to avoid the error
data <- data %>%
  mutate(across(c(Complete, Reuse, Access, Licence), as.character), .groups = "drop")  # Converts to character to ensure compatibility

# Reshape data to long format
long_data <- data %>%
  pivot_longer(cols = c(Complete, Reuse, Access, Licence), names_to = "Category", values_to = "Value") %>%
  mutate(Value = as.character(Value))  # Ensure that Value is a character

# Rename and reorder categories
long_data$Category <- recode(long_data$Category, 
                             "Complete" = "Completeness",
                             "Reuse" = "Reusability",
                             "Access" = "Accessibility",
                             "Licence" = "Licence")
long_data$Category <- factor(long_data$Category, levels = c("Completeness", "Reusability", "Accessibility", "Licence"))
long_data <- long_data %>%
  mutate(Value = factor(Value, levels = c("4", "3", "2", "1")))

# Calculate counts and percentages
long_data <- long_data %>%
  group_by(Category, Value) %>%
  summarise(Count = n(), .groups = "drop") %>%
  ungroup() %>%
  group_by(Category) %>%
  mutate(Total = sum(Count),
         Percentage = Count / Total * 100) %>%
  ungroup() %>%
  arrange(Category, desc(Value)) %>%
  group_by(Category) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), ""),
         CumPercentage = cumsum(Percentage) - (0.5 * Percentage)) %>%
  ungroup()

# Plot
plot_all <- ggplot(long_data, aes(x = Category, y = Percentage, fill = Value)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("#a04652", "#467fa0", "#7fa046", "#dec108")) +
  labs(y = "Percentage", fill = "Score") +
  theme_minimal() +
  geom_text(aes(label = ifelse(is.na(Percentage), "NA", ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")), 
                y = ifelse(is.na(Percentage), 0, Percentage)), 
            position = position_stack(vjust = 0.5), size = 6) +
  theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 18),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) +
  ylim(0, 100)

# Show the plot
print(plot_all)

# Save the plot
ggsave("plotall.png", plot_all, width = 10, height = 8, dpi = 300, bg="white")
```

# Reorder the levels of Completeness to match the desired order
```{r}

data$Complete <- factor(data$Complete, levels = c("4", "3", "2", "1"))

# Summarize data
summarized_data <- data %>%
  group_by(Institution, Complete) %>%
  summarise(Count = n(), .groups = "drop") %>%
  left_join(data %>% group_by(Institution) %>% summarise(Total = n(), .groups = "drop"), by = "Institution") %>%
  mutate(Percentage = Count/Total * 100)

# For cumulative percentages
summarized_data <- summarized_data %>%
  arrange(Institution, desc(Complete)) %>%
  group_by(Institution) %>%
  mutate(CumPercentage = cumsum(Percentage))

# Create the stacked bar chart for Completeness with colors and annotations
gcomp <- ggplot(summarized_data, aes(x = Institution, y = Percentage, fill = Complete)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("4" = "#a04652", "3" = "#467fa0", "2" = "#7fa046", "1" = "#dec108")) +
  labs(x = "Institution", y = "Percentage", fill = "Completeness Score") +
  theme_minimal() +
  geom_text(aes(label = ifelse(is.na(Percentage), "NA", ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")), 
                y = ifelse(is.na(Percentage), 0, Percentage)), 
            position = position_stack(vjust = 0.5), size = 5) +
  theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 18),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) +
  ylim(0, 100)

print(gcomp)
ggsave("gcomp.png", gcomp, width = 10, height = 8, dpi = 300)

```
```{r}

# Reorder the levels of Reuse to match the desired order
data$Reuse <- factor(data$Reuse, levels = c("4", "3", "2", "1"))

# Summarize data
summarized_data_reuse <- data %>%
  group_by(Institution, Reuse) %>%
  summarise(Count = n(), .groups = "drop") %>%
  left_join(data %>% group_by(Institution) %>% summarise(Total = n(), .groups = "drop"), by = "Institution") %>%
  mutate(Percentage = Count/Total * 100)

# For cumulative percentages
summarized_data_reuse <- summarized_data_reuse %>%
  arrange(Institution, desc(Reuse)) %>%
  group_by(Institution) %>%
  mutate(CumPercentage = cumsum(Percentage))

# Visualization
greuse <- ggplot(summarized_data_reuse, aes(x = Institution, y = Percentage, fill = Reuse)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("4" = "#a04652", "3" = "#467fa0", "2" = "#7fa046", "1" = "#dec108")) +
  labs(x = "Institution", y = "Percentage", fill = "Reusability Score") +
  theme_minimal() +
  geom_text(aes(label = ifelse(is.na(Percentage), "NA", ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")), 
                y = ifelse(is.na(Percentage), 0, Percentage)), 
            position = position_stack(vjust = 0.5), size = 5) +
  theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 18),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) +
  ylim(0, 100)

print(greuse)
ggsave("greuse.png", greuse, width = 10, height = 8, dpi = 300)
```
```{r}

# Reorder the levels of Accessibility to match the desired order
data$Access <- factor(data$Access, levels = c("4", "3", "2", "1"))

# Summarize data
summarized_data_Access <- data %>%
  group_by(Institution, Access) %>%
  summarise(Count = n(), .groups = "drop") %>%
  left_join(data %>% group_by(Institution) %>% summarise(Total = n(), .groups = "drop"), by = "Institution") %>%
  mutate(Percentage = Count/Total * 100)

# For cumulative percentages
summarized_data_Access <- summarized_data_Access %>%
  arrange(Institution, desc(Access)) %>%
  group_by(Institution) %>%
  mutate(CumPercentage = cumsum(Percentage))

# Visualization
gaccess <- ggplot(summarized_data_Access, aes(x = Institution, y = Percentage, fill = Access)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("4" = "#a04652", "3" = "#467fa0", "2" = "#7fa046", "1" = "#dec108")) +
  labs(x = "Institution", y = "Percentage", fill = "Accessibility Score") +
  theme_minimal() +
  geom_text(aes(label = ifelse(is.na(Percentage), "NA", ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")), 
                y = ifelse(is.na(Percentage), 0, Percentage)), 
            position = position_stack(vjust = 0.5), size = 5) +
  theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 18),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) +
  ylim(0, 100)

print(gaccess)
ggsave("gaccess.png", gaccess, width = 10, height = 8, dpi = 300)
```
```{r}
# Reorder the levels of License to match the desired order
data$Licence <- factor(data$Licence, levels = c("4", "3", "2", "1"))

# Summarize data
summarized_data_Licence <- data %>%
  group_by(Institution, Licence) %>%
  summarise(Count = n(), .groups = "drop") %>%
  left_join(data %>% group_by(Institution) %>% summarise(Total = n(), .groups = "drop"), by = "Institution") %>%
  mutate(Percentage = Count/Total * 100)

# For cumulative percentages
summarized_data_Licence <- summarized_data_Licence %>%
  arrange(Institution, desc(Licence)) %>%
  group_by(Institution) %>%
  mutate(CumPercentage = cumsum(Percentage))

# Visualization
glicence <- ggplot(summarized_data_Licence, aes(x = Institution, y = Percentage, fill = Licence)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("4" = "#a04652", "3" = "#467fa0", "2" = "#7fa046", "1" = "#dec108")) +
  labs(x = "Institution", y = "Percentage", fill = "Licensing Score") +
  theme_minimal() +
  geom_text(aes(label = ifelse(is.na(Percentage), "NA", ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")), 
                y = ifelse(is.na(Percentage), 0, Percentage)), 
            position = position_stack(vjust = 0.5), size = 5) +
  theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 18),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) +
  ylim(0, 100)

print(glicence)
ggsave("glicence.png", glicence, width = 10, height = 8, dpi = 300)
```

```{r}
install.packages("patchwork")  # Install the patchwork package
```


```{r}
library(patchwork)   

# Combine the plots using patchwork
combinedplot <- gcomp + greuse + gaccess + glicence +
   plot_layout(
    ncol = 2, heights = c(10, 10), widths = c(10, 10)
  ) +
  theme(axis.text.x = element_text(size = 10),  # Adjust x-axis text size
        axis.text.y = element_text(size = 10),  # Adjust y-axis text size
        strip.text = element_text(size = 10)) +  # Adjust strip text size
  geom_text(aes(label = sprintf("%.1f%%", Percentage), y = Percentage), size = 3)  # Adjust the size of the percentages

print(combinedplot)
ggsave("combinedplot.png", combinedplot, width = 15, height = 10, units = "in")


```



```{r}
# Reorder the levels of the 'Complete' variable
data$Complete <- factor(data$Complete, levels = c("4", "3", "2", "1"))

# Calculate the frequency and percentage for each 'Complete' score within each 'Year'
long_data <- data %>%
  count(Year, Complete) %>%
  group_by(Year) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - (0.5 * Percentage)) %>%
  ungroup()

# Custom colors
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Creating the stacked bar chart with percentage labels
Compyear <- ggplot(long_data, aes(x = as.factor(Year), y = n, fill = as.factor(Complete))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Percentage), 
    size = 4, 
    color = "black", 
    position = position_fill(vjust = 0.5)
  ) +
  scale_fill_manual(values = colors) +
  labs(x = "Year",
       y = "Percentage",
       fill = "Completeness") +
  theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 20),  # Increase axis text
    legend.title = element_text(size = 18), # Increase legend title
    legend.text = element_text(size = 20)  # Increase legend text
  ) +
  theme_minimal() +
  scale_x_discrete(name = "Year", labels = unique(long_data$Year)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(Compyear)

ggsave("Compyear.png", Compyear, width = 15, height = 10, units = "in", bg = "white")

```
```{r}
# Reorder the levels of the 'Reuse' variable
data$Reuse <- factor(data$Reuse, levels = c("4", "3", "2", "1"))

# Calculate the frequency and percentage for each 'Reuse' score within each 'Year'
long_data <- data %>%
  count(Year, Reuse) %>%
  group_by(Year) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - (0.5 * Percentage)) %>%
  ungroup()

# Custom colors
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Creating the stacked bar chart with percentage labels
reuseyear <- ggplot(long_data, aes(x = as.factor(Year), y = n, fill = as.factor(Reuse))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Percentage), 
    size = 4, 
    color = "black", 
    position = position_fill(vjust = 0.5)
  ) +
  scale_fill_manual(values = colors) +
  labs(x = "Year",
       y = "Percentage",
       fill = "Reusability") +
    theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 20),  # Increase axis text
    legend.title = element_text(size = 18), # Increase legend title
    legend.text = element_text(size = 20)  # Increase legend text
  ) +
  theme_minimal() +
  scale_x_discrete(name = "Year", labels = unique(long_data$Year)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(reuseyear)
ggsave("reuseyear.png", reuseyear, width = 15, height = 10, units = "in", bg = "white")


```


```{r}
# Reorder the levels of the 'Access' variable
data$Access <- factor(data$Access, levels = c("4", "3", "2", "1"))

# Calculate the frequency and percentage for each 'Access' score within each 'Year'
long_data <- data %>%
  count(Year, Access) %>%
  group_by(Year) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - (0.5 * Percentage)) %>%
  ungroup()

# Custom colors
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Creating the stacked bar chart with percentage labels
accessyear <- ggplot(long_data, aes(x = as.factor(Year), y = n, fill = as.factor(Access))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Percentage), 
    size = 4, 
    color = "black", 
    position = position_fill(vjust = 0.5)
  ) +
  scale_fill_manual(values = colors) +
  labs(x = "Year",
       y = "Percentage",
       fill = "Accessibility") +
  theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 20),  # Increase axis text
    legend.title = element_text(size = 18), # Increase legend title
    legend.text = element_text(size = 20)  # Increase legend text
  ) +
    theme_minimal() +
  scale_x_discrete(name = "Year", labels = unique(long_data$Year)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(accessyear)

ggsave("accessyear.png", accessyear, width = 15, height = 10, units = "in", bg = "white")

```
```{r}
# Reorder the levels of the 'Licence' variable
data$Licence <- factor(data$Licence, levels = c("4", "3", "2", "1"))

# Calculate the frequency and percentage for each 'Licence' score within each 'Year'
long_data <- data %>%
  count(Year, Licence) %>%
  group_by(Year) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - (0.5 * Percentage)) %>%
  ungroup()

# Custom colors
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Creating the stacked bar chart with percentage labels
licenceyear <- ggplot(long_data, aes(x = as.factor(Year), y = n, fill = as.factor(Licence))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Percentage), 
    size = 4, 
    color = "black", 
    position = position_fill(vjust = 0.5)
  ) +
  scale_fill_manual(values = colors) +
  labs(x = "Year",
       y = "Percentage",
       fill = "Licence") +
  theme(
    axis.title = element_text(size = 14), # Increase axis titles
    axis.text = element_text(size = 20),  # Increase axis text
    legend.title = element_text(size = 18), # Increase legend title
    legend.text = element_text(size = 20)  # Increase legend text
  ) +
  theme_minimal() +
  scale_x_discrete(name = "Year", labels = unique(long_data$Year)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(licenceyear)

ggsave("licenceyear.png", licenceyear, width = 15, height = 10, units = "in", bg = "white")

```


```{r}
plotyear <- Compyear + reuseyear + accessyear + licenceyear +
  plot_layout(
    ncol = 2, heights = c(10, 10), widths = c(10, 10)
  )
print(plotyear)

ggsave("plotyear.png", plotyear, width = 15, height = 10, units = "in")
 
```

#Create variables for FAIR principles in 2016 and COVID-19 in 2020
```{r}
data <- data %>%
  mutate(Period2020 = ifelse(Year <= 2020, "Before 2020", "After 2020")) %>%
  mutate(Period2016 = ifelse(Year <= 2016, "Before 2016", "After 2016")) %>%
  mutate(Period2020 = factor(Period2020, levels = c("Before 2020", "After 2020"))) # Adjust the order of factor levels

```


#Study the significant difference before and afterCovid-19 2020 using Median and Mann Whitney U test for all papers
```{r}
# Ensure the scoring criteria are numeric
data <- data %>%
  mutate(across(c(Complete, Reuse, Access, Licence), ~as.numeric(as.character(.))))

# Function to generate box plots for distribution checks
generate_boxplot <- function(data, score_var, period_var) {
  ggplot(data, aes_string(x = period_var, y = score_var, fill = period_var)) +
    geom_boxplot() +
    labs(title = paste("Boxplot for", score_var, "across", period_var),
         x = period_var,
         y = score_var) +
    theme_minimal()
}

# Generate and display box plots for each score with Period2020
lapply(c("Complete", "Reuse", "Access", "Licence"), function(score_var) {
  generate_boxplot(data, score_var, "Period2020")
})


# Function to perform the Mann-Whitney test and calculate medians
perform_analysis <- function(data, score_var, period_var) {
  test_result <- wilcox.test(reformulate(period_var, score_var), data = data, exact = FALSE)
  
  medians <- data %>%
    group_by(!!sym(period_var)) %>%
    summarise(median = median(!!sym(score_var), na.rm = TRUE), .groups = 'drop')
  
  list(median = medians, p.value = test_result$p.value)
}

# Analysis for each score for 2020
results_complete_2020 <- perform_analysis(data, "Complete", "Period2020")
results_reuse_2020 <- perform_analysis(data, "Reuse", "Period2020")
results_access_2020 <- perform_analysis(data, "Access", "Period2020")
results_licence_2020 <- perform_analysis(data, "Licence", "Period2020")

# Print results
print(results_complete_2020)
print(results_reuse_2020)
print(results_access_2020)
print(results_licence_2020)


#The assumptions of the test were met:
### Ordinal Data Check: The scoring criteria are ordinal variables
### Similar Distribution Shapes: For each criterion, it creates a box plot to visually inspect the distribution shapes. This is crucial to check if the distributions are similar across groups.
### Independence of Observations:the variables are independence observations. 
```
```{r}
library(ggplot2)
library(dplyr)
library(tidyr)

# Assuming 'data' already contains 'Year' and the scores as numeric
data <- data %>%
  mutate(Period2020 = ifelse(Year <= 2020, "Before 2020", "After 2020")) %>%
  mutate(Period2020 = factor(Period2020, levels = c("Before 2020", "After 2020")))

# Function to perform the Mann-Whitney test and return a data frame with significance labels
perform_analysis <- function(data, score_var, period_var) {
  test_result <- wilcox.test(reformulate(period_var, score_var), data = data, exact = FALSE)
  significance_label <- ifelse(test_result$p.value < 0.0001, "****",
                               ifelse(test_result$p.value < 0.001, "***",
                                      ifelse(test_result$p.value < 0.01, "**",
                                             ifelse(test_result$p.value < 0.05, "*", "ns"))))
  # Create a dataframe to return
  data_to_return <- data.frame(
    Score = score_var,
    Period = unique(data[[period_var]]),
    SignificanceLabel = significance_label
  )
  return(data_to_return)
}

# Apply to each score for Period2020 and combine results
significance_results <- lapply(c("Complete", "Reuse", "Access", "Licence"), function(score_var) {
  perform_analysis(data, score_var, "Period2020")
})
significance_data <- do.call(rbind, significance_results)

# Prepare plot data and merge with significance data
plot_data <- data %>%
  pivot_longer(cols = c(Complete, Reuse, Access, Licence), names_to = "Score", values_to = "Value") %>%
  left_join(significance_data, by = c("Score", "Period2020" = "Period"))

# Generate combined box plot
p <- ggplot(plot_data, aes(x = Period2020, y = Value, fill = Period2020)) +
  geom_boxplot() +
  facet_wrap(~ Score, scales = "free_y") +
  labs(title = "Boxplot Overview by COVID-19 Impact",
       x = "Period",
       y = "Score",
       fill = "COVID-19 Impact") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  geom_text(aes(label = SignificanceLabel, y = Inf), vjust = 1.5)

print(p)

# Save the plot
ggsave("COVID19_impact_boxplots.png", plot = p, width = 10, height = 8, dpi = 300)

```


#Study the significant difference before and after Fair 2016 using Median and Mann Whitney U for all the papers

```{r}
data <- data %>%
  mutate(Period2016 = ifelse(Year <= 2016, "Before 2016", "After 2016")) %>%
  mutate(Period2016 = factor(Period2016, levels = c("Before 2016", "After 2016"))) 

# Function to perform the Mann-Whitney test, calculate medians, and generate box plots
perform_analysis <- function(data, score_var, period_var) {
  test_result <- wilcox.test(reformulate(period_var, score_var), data = data, exact = FALSE)
  
  medians <- data %>%
    group_by(!!sym(period_var)) %>%
    summarise(median = median(!!sym(score_var), na.rm = TRUE), .groups = 'drop')
  
  # Generate a box plot
  box_plot <- ggplot(data, aes_string(x = period_var, y = score_var, fill = period_var)) +
    geom_boxplot() +
    labs(title = paste("Boxplot for", score_var, "across", period_var),
         x = period_var,
         y = score_var) +
    theme_minimal()
  
  # Print the box plot
  print(box_plot)
  
  list(median = medians, p.value = test_result$p.value, plot = box_plot)
}

# Analysis for each score for 2016
results_complete_2016 <- perform_analysis(data, "Complete", "Period2016")
results_reuse_2016 <- perform_analysis(data, "Reuse", "Period2016")
results_access_2016 <- perform_analysis(data, "Access", "Period2016")
results_licence_2016 <- perform_analysis(data, "Licence", "Period2016")

# Print results
results_complete_2016
results_reuse_2016
results_access_2016
results_licence_2016

```

#Now with FAIR plots showing significance values
```{r}
library(ggplot2)
library(dplyr)
library(tibble)

# Your existing data transformations for Period2016
data <- data %>%
  mutate(Period2016 = ifelse(Year <= 2016, "Before 2016", "After 2016")) %>%
  mutate(Period2016 = factor(Period2016, levels = c("Before 2016", "After 2016"))) 

# Enhanced function to perform the Mann-Whitney test, calculate medians, and generate annotated box plots
perform_analysis <- function(data, score_var, period_var) {
  test_result <- wilcox.test(reformulate(period_var, score_var), data = data, exact = FALSE)
  
  medians <- data %>%
    group_by(!!sym(period_var)) %>%
    summarise(median = median(!!sym(score_var), na.rm = TRUE), .groups = 'drop')
  
  # Determine significance notation based on p-value
  significance_label <- ifelse(test_result$p.value < 0.0001, "****",
                               ifelse(test_result$p.value < 0.001, "***",
                                      ifelse(test_result$p.value < 0.01, "**",
                                             ifelse(test_result$p.value < 0.05, "*", "ns"))))
  
  # Generate a box plot with significance annotation
  box_plot <- ggplot(data, aes_string(x = period_var, y = score_var, fill = period_var)) +
    geom_boxplot() +
    labs(title = paste("Boxplot for", score_var, "across", period_var),
         x = period_var,
         y = score_var) +
    theme_minimal() +
    geom_text(aes(label = significance_label, y = Inf), vjust = 1.5)
  
  # Print the box plot
  print(box_plot)
  
  list(median = medians, p.value = test_result$p.value, plot = box_plot)
}

# List of scores to analyze for 2016
score_vars <- c("Complete", "Reuse", "Access", "Licence")

# Analyze and plot each score for Period2016
results <- lapply(score_vars, function(score_var) {
  perform_analysis(data, score_var, "Period2016")
})

# Print results (optional, depending on your need to see results in console)
print(results)


```

#Combined into 4 panel figure with significance values for FAIR
```{r}
perform_analysis <- function(data, score_var, period_var) {
  test_result <- wilcox.test(reformulate(period_var, score_var), data = data, exact = FALSE)
  significance_label <- ifelse(test_result$p.value < 0.0001, "****",
                               ifelse(test_result$p.value < 0.001, "***",
                                      ifelse(test_result$p.value < 0.01, "**",
                                             ifelse(test_result$p.value < 0.05, "*", "ns"))))
  # Create a dataframe to return
  data_to_return <- data.frame(
    Score = score_var,
    Period = unique(data[[period_var]]),
    SignificanceLabel = significance_label
  )
  return(data_to_return)
}

# Apply to each score for Period2016
significance_results <- lapply(c("Complete", "Reuse", "Access", "Licence"), function(score_var) {
  perform_analysis(data, score_var, "Period2016")
})

# Combine all results into one dataframe
significance_data <- do.call(rbind, significance_results)

# Prepare plot data
plot_data <- data %>%
  pivot_longer(cols = c(Complete, Reuse, Access, Licence), names_to = "Score", values_to = "Value") %>%
  mutate(Period2016 = factor(ifelse(Year <= 2016, "Before 2016", "After 2016"), levels = c("Before 2016", "After 2016")))

# Merge with significance data
plot_data <- merge(plot_data, significance_data, by.x = c("Score", "Period2016"), by.y = c("Score", "Period"))


# Generate a combined box plot
p <- ggplot(plot_data, aes(x = Period2016, y = Value, fill = Period2016)) +
  geom_boxplot() +
  facet_wrap(~ Score, scales = "free_y") +
  labs(title = "Boxplot Overview by FAIR Implementation",
       x = "Period",
       y = "Score",
       fill = "FAIR Implementation") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  geom_text(aes(label = SignificanceLabel, y = Inf), vjust = 1.5)

# Print the plot
print(p)

# Assuming 'p' is your ggplot object
ggsave("plot.png", plot = p, width = 10, height = 8, dpi = 300)

```


#Study the significant difference before and after Fair 2016 using Median and Mann Whitney U test for the IIIR Papers

```{r}
perform_analysis <- function(data_IIIR, score_var, period_var) {
  test_result <- wilcox.test(reformulate(period_var, score_var), data = data_IIIR, exact = FALSE)
  
  medians <- data_IIIR %>%
    group_by(!!sym(period_var)) %>%
    summarise(median = median(!!sym(score_var), na.rm = TRUE), .groups = 'drop')
  
  list(median = medians, p.value = test_result$p.value)
}
# Analysis for each score for 2016
results_complete_2016_IIIR <- perform_analysis(data_IIIR, "Complete", "Period2016")
results_reuse_2016_IIIR <- perform_analysis(data_IIIR, "Reuse", "Period2016")
results_access_2016_IIIR <- perform_analysis(data_IIIR, "Access", "Period2016")
results_licence_2016_IIIR <- perform_analysis(data_IIIR, "Licence", "Period2016")

# Print results
results_complete_2016_IIIR
results_reuse_2016_IIIR
results_access_2016_IIIR
results_licence_2016_IIIR
```

#Study the significant difference before and after Fair 2016 using Median and Mann Whitney U test for the IQB3 Papers

```{r}
perform_analysis <- function(data_IQB3, score_var, period_var) {
  test_result <- wilcox.test(reformulate(period_var, score_var), data = data_IQB3, exact = FALSE)
  
  medians <- data_IQB3 %>%
    group_by(!!sym(period_var)) %>%
    summarise(median = median(!!sym(score_var), na.rm = TRUE), .groups = 'drop')
  
  list(median = medians, p.value = test_result$p.value)
}
# Analysis for each score for 2016
results_complete_2016_IQB3 <- perform_analysis(data_IQB3, "Complete", "Period2016")
results_reuse_2016_IQB3 <- perform_analysis(data_IQB3, "Reuse", "Period2016")
results_access_2016_IQB3 <- perform_analysis(data_IQB3, "Access", "Period2016")
results_licence_2016_IQB3 <- perform_analysis(data_IQB3, "Licence", "Period2016")

# Print results
results_complete_2016_IQB3
results_reuse_2016_IQB3
results_access_2016_IQB3
results_licence_2016_IQB3
```
#Study the significant difference before and after Fair 2016 using Median and Mann Whitney U test for the Roslin Papers

```{r}
perform_analysis <- function(data_Roslin, score_var, period_var) {
  test_result <- wilcox.test(reformulate(period_var, score_var), data = data_Roslin, exact = FALSE)
  
  medians <- data_Roslin %>%
    group_by(!!sym(period_var)) %>%
    summarise(median = median(!!sym(score_var), na.rm = TRUE), .groups = 'drop')
  
  list(median = medians, p.value = test_result$p.value)
}
# Analysis for each score for 2016
results_complete_2016_Roslin <- perform_analysis(data_Roslin, "Complete", "Period2016")
results_reuse_2016_Roslin <- perform_analysis(data_Roslin, "Reuse", "Period2016")
results_access_2016_Roslin <- perform_analysis(data_Roslin, "Access", "Period2016")
results_licence_2016_Roslin <- perform_analysis(data_Roslin, "Licence", "Period2016")

# Print results
results_complete_2016_Roslin
results_reuse_2016_Roslin
results_access_2016_Roslin
results_licence_2016_Roslin
```
#plot the distribution of DAS for each completeness score
```{r}
# Transform the DAS variable
data <- data %>%
  mutate(NewDAS = case_when(
    is.na(DAS) ~ "Not Presented",
    DAS == 1 ~ "Shared",
    DAS == 0 ~ "Not Shared",
    TRUE ~ as.character(DAS)))  # This line is just a fallback to handle unexpected values

class(data$NewDAS)

# Convert the new DAS variable to a factor for plotting
data$NewDAS <- factor(data$NewDAS, levels = c("Not Presented", "Not Shared", "Shared"))

# Calculate frequency and percentage for each combination of 'Complete' score and 'NewDAS' status
das_frequency <- data %>%
  group_by(Complete, NewDAS) %>%
  summarise(Frequency = n(), .groups = 'drop') %>%
  mutate(Percentage = (Frequency / sum(Frequency)) * 100)

# Define specific colors for the new DAS values
colors <- c("Not Presented" = "gray", "Not Shared" = "#CD6600", "Shared" = "#008B8B")

# Create the plot
das_plot <- ggplot(das_frequency, aes(x = as.factor(Complete), y = Frequency, fill = NewDAS)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = paste0(round(Percentage, 1), "% (", Frequency, ")")),
            position = position_dodge(width = 0.9), vjust = -0.25, size = 3.5) +
  scale_fill_manual(values = colors, name = "DAS Status") +
  theme_bw() +  # Use theme_bw for a white background
   theme(
    axis.title = element_text(size = 10), # Increase axis titles
    axis.text = element_text(size = 16),  # Increase axis text
    legend.title = element_text(size = 14), # Increase legend title
    legend.text = element_text(size = 16)  # Increase legend text
  ) +
  labs(x = "Completeness Score", y = "Frequency") +
  theme(legend.position = "bottom")

# Print and save the plot
print(das_plot)
ggsave("new_das_distribution_plot.png", das_plot, width = 8, height = 6,  bg = "white")

```


#plot the distribution of DAS for each accessibility score
```{r}
# Transform the DAS variable
data <- data %>%
  mutate(NewDAS = case_when(
    is.na(DAS) ~ "Not Presented",
    DAS == 1 ~ "Shared",
    DAS == 0 ~ "Not Shared",
    TRUE ~ as.character(DAS)))  # This line is just a fallback to handle unexpected values

class(data$NewDAS)

# Convert the new DAS variable to a factor for plotting
data$NewDAS <- factor(data$NewDAS, levels = c("Not Presented", "Not Shared", "Shared"))

# Calculate frequency and percentage for each combination of 'Access' score and 'NewDAS' status
das_frequency <- data %>%
  group_by(Access, NewDAS) %>%
  summarise(Frequency = n(), .groups = 'drop') %>%
  mutate(Percentage = (Frequency / sum(Frequency)) * 100)

# Define specific colors for the new DAS values
colors <- c("Not Presented" = "gray", "Not Shared" = "#CD6600", "Shared" = "#008B8B")

# Create the plot
das_plot <- ggplot(das_frequency, aes(x = as.factor(Access), y = Frequency, fill = NewDAS)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = paste0(round(Percentage, 1), "% (", Frequency, ")")),
            position = position_dodge(width = 0.9), vjust = -0.25, size = 3.5) +
  scale_fill_manual(values = colors, name = "DAS Status") +
  theme_bw() +  # Use theme_bw for a white background
   theme(
    axis.title = element_text(size = 10), # Increase axis titles
    axis.text = element_text(size = 16),  # Increase axis text
    legend.title = element_text(size = 14), # Increase legend title
    legend.text = element_text(size = 16)  # Increase legend text
  ) +
  labs(x = "Accessibility Score", y = "Frequency") +
  theme(legend.position = "bottom")

# Print and save the plot
print(das_plot)
ggsave("Access_new_das_distribution_plot.png", das_plot, width = 8, height = 6,  bg = "white")

```


```{r}
# Calculate the total count per year
yearly_totals <- data %>%
  group_by(Year) %>%
  summarise(Total = n(), .groups = 'drop')

# Join the totals back to the original data and calculate proportions
proportion_data <- data %>%
  left_join(yearly_totals, by = "Year") %>%
  group_by(Year, NewDAS) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  mutate(Proportion = Count / n())

# Get all unique years for the x-axis
all_years <- sort(unique(proportion_data$Year))

colors <- c("Not Presented" = "#828282", "Not Shared" = "#CD6600", "Shared" = "#008B8B")

# Plot with every year on the x-axis
DASyear <- ggplot(proportion_data, aes(x = Year, y = Proportion, color = NewDAS, group = NewDAS)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 2016, linetype = "dashed", color = "black") +
  geom_vline(xintercept = 2020, linetype = "dashed", color = "black") +
  annotate("text", x = 2016, y = max(proportion_data$Proportion), label = "FAIR Principle", angle = 90, hjust = 1.5, vjust = -0.5, size = 3.5) +
  annotate("text", x = 2020, y = max(proportion_data$Proportion), label = "COVID-19 Pandemic", angle = 90, hjust = 1.5, vjust = 1, size = 3.5) +
  scale_x_continuous(breaks = all_years) +
  labs(x = "Year", y = "Proportion", color = "Data Availability Statement") +
  theme(
    axis.title = element_text(size = 10), # Increase axis titles
    axis.text = element_text(size = 16),  # Increase axis text
    legend.title = element_text(size = 14), # Increase legend title
    legend.text = element_text(size = 16)  # Increase legend text
  ) +
  theme_minimal() +
  scale_color_manual(values = colors)
 
print(DASyear)

ggsave("DASyear.png", DASyear, width = 8, height = 6,  bg = "white")

```

```{r}
 #Convert the Preprint numeric values to factor levels 'Yes' and 'No'
data$Preprints <- factor(ifelse(data$Preprints == 1, "Yes", "No"))

# Calculate the total count per year and proportion for Preprint
proportion_data <- data %>%
  group_by(Year) %>%
  count(Preprints) %>%
  mutate(Total = sum(n), 
         Proportion = n / Total)

# Get all unique years for the x-axis
all_years <- sort(unique(proportion_data$Year))

# Define colors for 'Yes' and 'No'
colors <- c("No" = "#CD6600", "Yes" = "#008B8B")

# Plot with every year on the x-axis for Preprints
PreprintYear <- ggplot(proportion_data, aes(x = Year, y = Proportion, color = Preprints, group = Preprints)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 2016, linetype = "dashed", color = "black") +
  geom_vline(xintercept = 2020, linetype = "dashed", color = "black") +
  annotate("text", x = 2016, y = max(proportion_data$Proportion), label = "FAIR Principle", angle = 90, hjust = 1.5, vjust = -0.5, size = 3.5) +
  annotate("text", x = 2020, y = max(proportion_data$Proportion), label = "COVID-19 Pandemic", angle = 90, hjust = 1.5, vjust = 1, size = 3.5) +
  scale_x_continuous(breaks = all_years) +
  labs(x = "Year", y = "Proportion", color = "Preprints") +
  theme(
    axis.title = element_text(size = 10), # Increase axis titles
    axis.text = element_text(size = 16),  # Increase axis text
    legend.title = element_text(size = 14), # Increase legend title
    legend.text = element_text(size = 16)  # Increase legend text
  ) +
  theme_minimal() +
  scale_color_manual(values = colors)
 
print(PreprintYear)

# Save the plot for Preprint
ggsave("PreprintYear.png", PreprintYear, width = 8, height = 6, bg = "white")
```

#study if FAIR implementation and Covid19 have an effect on DAS and Preprint

# 1- Fair implementataion
```{r}
total_by_period <- data %>%
  group_by(Period2016) %>%
  summarise(Total = n(), .groups = "drop")

# Join this total with the DAS and Preprints summaries and calculate the percentage
Preprints_summary <- data %>%
  group_by(Period2016, Preprints) %>%
  summarise(Frequency = n(), .groups = "drop") %>%
  left_join(total_by_period, by = "Period2016") %>%
  mutate(Percentage = (Frequency / Total) * 100)

# Print the Preprints summary table
print(Preprints_summary)

Preprints_table <- table(data$Preprints, data$Period2016)

# Perform the Chi-square test
result <- chisq.test(Preprints_table)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values) ##in order to check the assumption of chi square test: Each cell in the contingency table should have an expected count of 5 or more. 

#The assumptions of that test were met:

#### Sample Size: Each cell in the contingency table has an expected count of 5 or more. 
#### Independence: The observations are  independent of each other. This means that the selection of one observation should not influence or affect the selection of another observation.
### Random Sampling: The data is a random sample.
### Categorical Data: Both variables should be categorical (either nominal or ordinal).
```

```{r}
total_by_period <- data %>%
  group_by(Period2016) %>%
  summarise(Total = n(), .groups = "drop")

# Join this total with the NewDAS and Preprints summaries and calculate the percentage
das_summary <- data %>%
  group_by(Period2016, NewDAS)%>%
  summarise(Frequency = n(), .groups = "drop") %>%
  left_join(total_by_period, by = "Period2016") %>%
  mutate(Percentage = (Frequency / Total) * 100)

# Print the DAS summary table
print(das_summary)

das_table <- table(data$NewDAS, data$Period2016)

# Perform the Chi-square test
result <- chisq.test(das_table)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values) ##in order to check the assumption of chi square test: Each cell in the contingency table should have an expected count of 5 or more. 

#The assumptions of that test were met:

#### Sample Size: Each cell in the contingency table has an expected count of 5 or more. 
#### Independence: The observations are  independent of each other. This means that the selection of one observation should not influence or affect the selection of another observation.
### Random Sampling: The data is a random sample.
### Categorical Data: Both variables should be categorical (either nominal or ordinal).
```
#count the total number of papers per year event
```{r}
# Filter papers published between 2014 and 2016
papers_between_2014_and_2016 <- subset(data, Year >= 2021 & Year <= 2023)

# Get the count of papers
count_papers_between_2014_and_2016 <- nrow(papers_between_2014_and_2016)

# Print the count
print(count_papers_between_2014_and_2016)
```

#2- Covid 19
# the impact of COVID-19 on Preprints
```{r}
total_by_period <- data %>%
  group_by(Period2020)%>%
  summarise(Total = n(), .groups = "drop")

# Join this total with the DAS and Preprints summaries and calculate the percentage
Preprints_summary <- data %>%
  group_by(Period2020, Preprints) %>%
  summarise(Frequency = n(), .groups = "drop") %>%
  left_join(total_by_period, by = "Period2020") %>%
  mutate(Percentage = (Frequency / Total) * 100)

# Print the Preprints summary table
print(Preprints_summary)

Preprints_table <- table(data$Preprints, data$Period2020)

# Perform the Chi-square test
result <- chisq.test(Preprints_table)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values) ##in order to check the assumption of chi square test: Each cell in the contingency table should have an expected count of 5 or more. 

#The assumptions of that test were met:

#### Sample Size: Each cell in the contingency table has an expected count of 5 or more. 
#### Independence: The observations are  independent of each other. This means that the selection of one observation should not influence or affect the selection of another observation.
### Random Sampling: The data is a random sample.
### Categorical Data: Both variables should be categorical (either nominal or ordinal)
```
# the impact of COVID-19 on DAS

```{r}
total_by_period <- data %>%
  group_by(Period2020) %>%
  summarise(Total = n(), .groups = "drop")

# Join this total with the NewDAS and Preprints summaries and calculate the percentage
das_summary <- data %>%
  group_by(Period2020, NewDAS)%>%
  summarise(Frequency = n(), .groups = "drop") %>%
  left_join(total_by_period, by = "Period2020") %>%
  mutate(Percentage = (Frequency / Total) * 100)

# Print the DAS summary table
print(das_summary)

das_table <- table(data$NewDAS, data$Period2020)

# Perform the Chi-square test
result <- chisq.test(das_table)

#Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values) ##in order to check the assumption of chi square test: Each cell in the contingency table should have an expected count of 5 or more. 

#The assumptions of that test were met:

#### Sample Size: Each cell in the contingency table has an expected count of 5 or more. 
#### Independence: The observations are  independent of each other. This means that the selection of one observation should not influence or affect the selection of another observation.
### Random Sampling: The data is a random sample.
### Categorical Data: Both variables should be categorical (either nominal or ordinal)

```


#IMPACT OF COVID, in isolation, on DAS
```{r}
# Filter the data to include only papers from 2018 onwards
data_filtered <- data %>%
  filter(Year >= 2018) %>%
  mutate(Period2018_2023 = ifelse(Year <= 2020, "2018-2020", "2021-2023"))

# Summarise total papers by the new period 2018-2023
total_by_period_2018_2023 <- data_filtered %>%
  group_by(Period2018_2023) %>%
  summarise(Total = n(), .groups = "drop")

# Join this total with the NewDAS and Preprints summaries for the new period and calculate the percentage
das_summary_2018_2023 <- data_filtered %>%
  group_by(Period2018_2023, NewDAS) %>%
  summarise(Frequency = n(), .groups = "drop") %>%
  left_join(total_by_period_2018_2023, by = "Period2018_2023") %>%
  mutate(Percentage = (Frequency / Total) * 100)

# Print the DAS summary table for the new period
print(das_summary_2018_2023)

# Create a contingency table for the new period
das_table_2018_2023 <- table(data_filtered$NewDAS, data_filtered$Period2018_2023)

# Perform the Chi-square test for the new period
result_2018_2023 <- chisq.test(das_table_2018_2023)

# Print the chi-square results for the new period
print(result_2018_2023)

# Get the expected values for the chi-square test for the new period
expected_values_2018_2023 <- result_2018_2023$expected

# Print the expected values to check the assumption of the chi-square test
print(expected_values_2018_2023)

```

#2 - Ordinal regression models (Scoring Criteria and year)

```{r}
library(ordinal) # ordinal logistic regression: cumulative link mixed models, clm function is in this package; For a detailed explanation of the package and the functions available see: https://cran.r-project.org/web/packages/ordinal/vignettes/clm_article.pdf
library(VGAM) # more ordinal regression 
library(dplyr) # Data manipulation
library(chisq.posthoc.test) # If needed
library(gmodels) # For SPSS style chi-sq/ contingency tables
library(emmeans)
library(sure)
```

```{r}
data$Complete <- factor(data$Complete, ordered = TRUE)
data$Reuse <- factor(data$Reuse, ordered = TRUE)
data$Access <- factor(data$Access, ordered = TRUE)
data$Licence <- factor(data$Licence, ordered = TRUE)
```


#Ordinal regression model for the Completeness by year

```{r}
m1a <- clm(Complete ~ Year, data = data)
summary_m1a <- summary(m1a) 


# Extract coefficients and standard errors
coefs <- summary(m1a)$coefficients
OR <- exp(coefs[,1])
lower_ci <- exp(coefs[,1] - 1.96 * coefs[,2])
upper_ci <- exp(coefs[,1] + 1.96 * coefs[,2])

# Calculate z-values and p-values
z_values <- coefs[,1] / coefs[,2]
p_values <- 2 * (1 - pnorm(abs(z_values)))

# Combine everything into a data frame for easy viewing
results <- data.frame(OR = OR, LowerCI = lower_ci, UpperCI = upper_ci, p_value = p_values)
print(results)

#Check the Assumptions 
nominal_test(m1a) # This is a test of the proportional odds assumption, odds are proportional in this case.
scale_test(m1a) # scale test checks for equal variance/ scale across year, assumptions are not violated here because p = 0.8386. If p is less than 0.05 then assumptions are violated
convergence(m1a) # This is another way to assess the model 

######## Graphically validate proportional odds using the sure package #######

autoplot.clm(m1a, what = c("qq")) # most of the points fall along the line, so no violation of linearity
autoplot.clm(m1a, what = c("fitted")) # the residuals do not show a clear pattern or trend
autoplot.clm(m1a, what = c("covariate"), x = data$Year) # scale test indicated no variance issues. This residual, covariate plots seems acceptable as well
# N.B. - Interpreting residual plots is largely subjective! 

###plotting the model
# Predict probabilities for each level of 'Complete'
new_data <- data.frame(Year = sort(unique(data$Year)))
pred_probs <- predict(m1a, newdata = new_data, type = "prob")

# Add the 'Year' column to the predicted probabilities dataframe
pred_probs_df <- cbind(new_data, as.data.frame(pred_probs))

# Convert the predicted probabilities to a long format for ggplot
library(reshape2)
pred_probs_long <- melt(pred_probs_df, id.vars = 'Year', variable.name = 'CompleteLevel', value.name = 'PredictedProbability')

# Set the colors
my_colors <- c("#E5696F", "#50689B", "#36D0A1", "#D0A136")

# Modify ggplot command
og1 <- ggplot(pred_probs_long, aes(x = Year, y = PredictedProbability, group = CompleteLevel, color = CompleteLevel)) + 
  geom_line(size = 2) +  # Set size of the lines to make them thicker
  scale_color_manual(values = my_colors) +
  scale_x_continuous(breaks = unique(pred_probs_long$Year)) +  # Show all years on x-axis
  labs(x = "Year", y = "Probability", color = "Completeness Score") +
  theme(
    axis.title = element_text(size = 12), # Increase axis titles
    axis.text = element_text(size = 14),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) 
print(og1)

ggsave("og1.png", og1, width = 15, height = 10, units = "in",  bg = "white")

```


#Ordinal regression model for the Reusibility by year

```{r}
m1a <- clm(Reuse ~ Year, data = data) 
summary_m1a <- summary(m1a) 


# Extract coefficients and standard errors
coefs <- summary(m1a)$coefficients
OR <- exp(coefs[,1])
lower_ci <- exp(coefs[,1] - 1.96 * coefs[,2])
upper_ci <- exp(coefs[,1] + 1.96 * coefs[,2])

# Calculate z-values and p-values
z_values <- coefs[,1] / coefs[,2]
p_values <- 2 * (1 - pnorm(abs(z_values)))

# Combine everything into a data frame for easy viewing
results <- data.frame(OR = OR, LowerCI = lower_ci, UpperCI = upper_ci, p_value = p_values)
print(results)


#Check the Assumptions 
nominal_test(m1a) # This is a test of the proportional odds assumption, odds are proportional in this case.
scale_test(m1a) # scale test checks for equal variance/ scale across year, assumptions are not violated here
convergence(m1a) # This is another way to assess the model 

######## Graphically validate proportional odds using the sure package #######

autoplot.clm(m1a, what = c("qq")) # most of the points fall along the line, so no violation of linearity
autoplot.clm(m1a, what = c("fitted")) # the residuals do not show a clear pattern or trend
autoplot.clm(m1a, what = c("covariate"), x = data$Year) # scale test indicated no variance issues. This residual, covariate plots seems acceptable as well
# N.B. - Interpreting residual plots is largely subjective! 

###plotting the model
# Predict probabilities for each level of 'Complete'
new_data <- data.frame(Year = sort(unique(data$Year)))
pred_probs <- predict(m1a, newdata = new_data, type = "prob")

# Add the 'Year' column to the predicted probabilities dataframe
pred_probs_df <- cbind(new_data, as.data.frame(pred_probs))

# Convert the predicted probabilities to a long format for ggplot
library(reshape2)
pred_probs_long <- melt(pred_probs_df, id.vars = 'Year', variable.name = 'Reuselevel', value.name = 'PredictedProbability')

# Set the colors 
my_colors <- c("#E5696F", "#50689B", "#36D0A1", "#D0A136")

# Modify ggplot command
og2 <- ggplot(pred_probs_long, aes(x = Year, y = PredictedProbability, group = Reuselevel, color = Reuselevel)) + 
  geom_line(size = 2) +  # Set size of the lines to make them thicker
  scale_color_manual(values = my_colors) +  # Use colors
  scale_x_continuous(breaks = unique(pred_probs_long$Year)) +  # Show all years on x-axis
  labs(x = "Year", y = "Probability", color = "Reusability Score") +
  theme(
    axis.title = element_text(size = 12), # Increase axis titles
    axis.text = element_text(size = 14),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) 
print(og2)

ggsave("og2.png", og2, width = 15, height = 10, units = "in",  bg = "white")

```

```{r}
m1a <- clm(Access ~ Year, data = data) 
summary_m1a <- summary(m1a) 


# Extract coefficients and standard errors
coefs <- summary_m1a$coefficients[, 1]  # Coefficients
se_coefs <- summary_m1a$coefficients[, 2]  # Standard errors

# Extract coefficients and standard errors
coefs <- summary(m1a)$coefficients
OR <- exp(coefs[,1])
lower_ci <- exp(coefs[,1] - 1.96 * coefs[,2])
upper_ci <- exp(coefs[,1] + 1.96 * coefs[,2])

# Calculate z-values and p-values
z_values <- coefs[,1] / coefs[,2]
p_values <- 2 * (1 - pnorm(abs(z_values)))

# Combine everything into a data frame for easy viewing
results <- data.frame(OR = OR, LowerCI = lower_ci, UpperCI = upper_ci, p_value = p_values)
print(results)

#Check the Assumptions 
nominal_test(m1a) # This is a test of the proportional odds assumption, odds are proportional in this case.
scale_test(m1a) # scale test checks for equal variance/ scale across year, assumptions are not violated here
convergence(m1a) # This is another way to assess the model 

######## Graphically validate proportional odds using the sure package #######

autoplot.clm(m1a, what = c("qq")) # most of the points fall along the line, so no violation of linearity
autoplot.clm(m1a, what = c("fitted")) # the residuals do not show a clear pattern or trend
autoplot.clm(m1a, what = c("covariate"), x = data$Year) # scale test indicated no variance issues. This residual, covariate plots seems acceptable as well
# N.B. - Interpreting residual plots is largely subjective! 

###plotting the model
# Predict probabilities for each level of 'Complete'
new_data <- data.frame(Year = sort(unique(data$Year)))
pred_probs <- predict(m1a, newdata = new_data, type = "prob")

# Add the 'Year' column to the predicted probabilities dataframe
pred_probs_df <- cbind(new_data, as.data.frame(pred_probs))

# Convert the predicted probabilities to a long format for ggplot
library(reshape2)
pred_probs_long <- melt(pred_probs_df, id.vars = 'Year', variable.name = 'AccessLevel', value.name = 'PredictedProbability')

# Set the colors
my_colors <- c("#E5696F", "#50689B", "#36D0A1", "#D0A136")

# Modify ggplot command
og3 <- ggplot(pred_probs_long, aes(x = Year, y = PredictedProbability, group = AccessLevel, color = AccessLevel)) + 
  geom_line(size = 2) +  # Set size of the lines to make them thicker
  scale_color_manual(values = my_colors) +  # Use colors
  scale_x_continuous(breaks = unique(pred_probs_long$Year)) +  # Show all years on x-axis
  labs(x = "Year", y = "Probability", color = "Accessibility Score") +
  theme(
    axis.title = element_text(size = 12), # Increase axis titles
    axis.text = element_text(size = 14),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) 
print(og3)

ggsave("og3.png", og3, width = 15, height = 10, units = "in",  bg = "white")

```

```{r}
m1a <- clm(Licence ~ Year, data = data) 
summary_m1a <- summary(m1a) 


# Extract coefficients and standard errors
coefs <- summary_m1a$coefficients[, 1]  # Coefficients
se_coefs <- summary_m1a$coefficients[, 2]  # Standard errors

# Extract coefficients and standard errors
coefs <- summary(m1a)$coefficients
OR <- exp(coefs[,1])
lower_ci <- exp(coefs[,1] - 1.96 * coefs[,2])
upper_ci <- exp(coefs[,1] + 1.96 * coefs[,2])

# Calculate z-values and p-values
z_values <- coefs[,1] / coefs[,2]
p_values <- 2 * (1 - pnorm(abs(z_values)))

# Combine everything into a data frame for easy viewing
results <- data.frame(OR = OR, LowerCI = lower_ci, UpperCI = upper_ci, p_value = p_values)
print(results)

#Check the Assumptions 
nominal_test(m1a) # This is a test of the proportional odds assumption, odds are proportional in this case.
scale_test(m1a) # scale test checks for equal variance/ scale across year, assumptions are not violated here
convergence(m1a) # This is another way to assess the model 

######## Graphically validate proportional odds using the sure package #######

autoplot.clm(m1a, what = c("qq")) # most of the points fall along the line, so no violation of linearity
autoplot.clm(m1a, what = c("fitted")) # the residuals do not show a clear pattern or trend
autoplot.clm(m1a, what = c("covariate"), x = data$Year) # scale test indicated no variance issues. This residual, covariate plots seems acceptable as well
# N.B. - Interpreting residual plots is largely subjective! 

###plotting the model
# Predict probabilities for each level of 'Complete'
new_data <- data.frame(Year = sort(unique(data$Year)))
pred_probs <- predict(m1a, newdata = new_data, type = "prob")

# Add the 'Year' column to the predicted probabilities dataframe
pred_probs_df <- cbind(new_data, as.data.frame(pred_probs))

# Convert the predicted probabilities to a long format for ggplot
library(reshape2)
pred_probs_long <- melt(pred_probs_df, id.vars = 'Year', variable.name = 'Licencelevel', value.name = 'PredictedProbability')

# Set the colors 
my_colors <- c("#E5696F", "#50689B", "#36D0A1", "#D0A136")

# Modify ggplot command
og4 <- ggplot(pred_probs_long, aes(x = Year, y = PredictedProbability, group = Licencelevel, color = Licencelevel)) + 
  geom_line(size = 2) +  # Set size of the lines to make them thicker
  scale_color_manual(values = my_colors) +  # Use colors
  scale_x_continuous(breaks = unique(pred_probs_long$Year)) +  # Show all years on x-axis
  labs(x = "Year", y = "Probability", color = "Licence Score") +
  theme(
    axis.title = element_text(size = 12), # Increase axis titles
    axis.text = element_text(size = 14),  # Increase axis text
    legend.title = element_text(size = 12), # Increase legend title
    legend.text = element_text(size = 14)  # Increase legend text
  ) 
print(og4)

ggsave("og4.png", og4, width = 15, height = 10, units = "in",  bg = "white")

```
```{r}
ordinalplot <- og1 + og2 + og3 + og4 +
  plot_layout(
    ncol = 2, heights = c(10, 10), widths = c(10, 10)
  )
print(ordinalplot)

ggsave("ordinalplot.png", ordinalplot, width = 15, height = 10, units = "in", bg="white")
 
```


####3

#Ordinal Regression  models depending on the sharing projects

```{r}
m1a <- clm(Complete ~ NewDAS + Preprints, data = data) 
summary_m1a <- summary(m1a)

# Extract coefficients and standard errors
coefs <- summary_m1a$coefficients[, 1]  # Coefficients
se_coefs <- summary_m1a$coefficients[, 2]  # Standard errors

# Extract coefficients and standard errors
coefs <- summary(m1a)$coefficients
OR <- exp(coefs[,1])
lower_ci <- exp(coefs[,1] - 1.96 * coefs[,2])
upper_ci <- exp(coefs[,1] + 1.96 * coefs[,2])

# Calculate z-values and p-values
z_values <- coefs[,1] / coefs[,2]
p_values <- 2 * (1 - pnorm(abs(z_values)))

# Combine everything into a data frame for easy viewing
results <- data.frame(OR = OR, LowerCI = lower_ci, UpperCI = upper_ci, p_value = p_values)
print(results)


#Check the Assumptions 
nominal_test(m1a) # This is a test of the proportional odds assumption, odds are proportional in this case.
scale_test(m1a) # scale test checks for equal variance/ scale across year, assumptions are not violated here because p = 0.8386. If p is less than 0.05 then assumptions are violated
convergence(m1a) # This is another way to assess the model 

######## Graphically validate proportional odds using the sure package #######

autoplot.clm(m1a, what = c("qq")) # most of the points fall along the line, so no violation of linearity
autoplot.clm(m1a, what = c("fitted")) # the residuals do not show a clear pattern or trend
autoplot.clm(m1a, what = c("covariate"), x = data$NewDAS) # scale test indicated no variance issues. This residual, covariate plots seems acceptable as well
autoplot.clm(m1a, what = c("covariate"), x = data$Preprints)
# N.B. - Interpreting residual plots is largely subjective! 

```


```{r}
m1a <- clm(Reuse ~ NewDAS + Preprints, data = data) 
summary_m1a <- summary(m1a)

# Extract coefficients and standard errors
coefs <- summary_m1a$coefficients[, 1]  # Coefficients
se_coefs <- summary_m1a$coefficients[, 2]  # Standard errors

# Extract coefficients and standard errors
coefs <- summary(m1a)$coefficients
OR <- exp(coefs[,1])
lower_ci <- exp(coefs[,1] - 1.96 * coefs[,2])
upper_ci <- exp(coefs[,1] + 1.96 * coefs[,2])

# Calculate z-values and p-values
z_values <- coefs[,1] / coefs[,2]
p_values <- 2 * (1 - pnorm(abs(z_values)))

# Combine everything into a data frame for easy viewing
results <- data.frame(OR = OR, LowerCI = lower_ci, UpperCI = upper_ci, p_value = p_values)
print(results)


#Check the Assumptions 
nominal_test(m1a) # This is a test of the proportional odds assumption, odds are proportional in this case.
scale_test(m1a) # scale test checks for equal variance/ scale across year, assumptions are not violated here because p = 0.8386. If p is less than 0.05 then assumptions are violated
convergence(m1a) # This is another way to assess the model 

######## Graphically validate proportional odds using the sure package #######

autoplot.clm(m1a, what = c("qq")) # most of the points fall along the line, so no violation of linearity
autoplot.clm(m1a, what = c("fitted")) # the residuals do not show a clear pattern or trend
autoplot.clm(m1a, what = c("covariate"), x = data$NewDAS) # scale test indicated no variance issues. This residual, covariate plots seems acceptable as well
autoplot.clm(m1a, what = c("covariate"), x = data$Preprints)
# N.B. - Interpreting residual plots is largely subjective! 
```

```{r}
m1a <- clm(Access ~ NewDAS + Preprints, data = data) 
summary_m1a <- summary(m1a)

# Extract coefficients and standard errors
coefs <- summary_m1a$coefficients[, 1]  # Coefficients
se_coefs <- summary_m1a$coefficients[, 2]  # Standard errors

# Extract coefficients and standard errors
coefs <- summary(m1a)$coefficients
OR <- exp(coefs[,1])
lower_ci <- exp(coefs[,1] - 1.96 * coefs[,2])
upper_ci <- exp(coefs[,1] + 1.96 * coefs[,2])

# Calculate z-values and p-values
z_values <- coefs[,1] / coefs[,2]
p_values <- 2 * (1 - pnorm(abs(z_values)))

# Combine everything into a data frame for easy viewing
results <- data.frame(OR = OR, LowerCI = lower_ci, UpperCI = upper_ci, p_value = p_values)
print(results)


#Check the Assumptions 
nominal_test(m1a) # This is a test of the proportional odds assumption, odds are proportional in this case.
scale_test(m1a) # scale test checks for equal variance/ scale across year, assumptions are not violated here because p = 0.8386. If p is less than 0.05 then assumptions are violated
convergence(m1a) # This is another way to assess the model 

######## Graphically validate proportional odds using the sure package #######

autoplot.clm(m1a, what = c("qq")) # most of the points fall along the line, so no violation of linearity
autoplot.clm(m1a, what = c("fitted")) # the residuals do not show a clear pattern or trend
autoplot.clm(m1a, what = c("covariate"), x = data$NewDAS) # scale test indicated no variance issues. This residual, covariate plots seems acceptable as well
autoplot.clm(m1a, what = c("covariate"), x = data$Preprints)
# N.B. - Interpreting residual plots is largely subjective! 
```


```{r}
m1a <- clm(Licence ~ NewDAS + Preprints, data = data) 
summary_m1a <- summary(m1a)

# Extract coefficients and standard errors
coefs <- summary_m1a$coefficients[, 1]  # Coefficients
se_coefs <- summary_m1a$coefficients[, 2]  # Standard errors

# Extract coefficients and standard errors
coefs <- summary(m1a)$coefficients
OR <- exp(coefs[,1])
lower_ci <- exp(coefs[,1] - 1.96 * coefs[,2])
upper_ci <- exp(coefs[,1] + 1.96 * coefs[,2])

# Calculate z-values and p-values
z_values <- coefs[,1] / coefs[,2]
p_values <- 2 * (1 - pnorm(abs(z_values)))

# Combine everything into a data frame for easy viewing
results <- data.frame(OR = OR, LowerCI = lower_ci, UpperCI = upper_ci, p_value = p_values)
print(results)


#Check the Assumptions 
nominal_test(m1a) # This is a test of the proportional odds assumption, odds are proportional in this case.
scale_test(m1a) # scale test checks for equal variance/ scale across year, assumptions are not violated here because p = 0.8386. If p is less than 0.05 then assumptions are violated
convergence(m1a) # This is another way to assess the model 

######## Graphically validate proportional odds using the sure package #######

autoplot.clm(m1a, what = c("qq")) # most of the points fall along the line, so no violation of linearity
autoplot.clm(m1a, what = c("fitted")) # the residuals do not show a clear pattern or trend
autoplot.clm(m1a, what = c("covariate"), x = data$NewDAS) # scale test indicated no variance issues. This residual, covariate plots seems acceptable as well
autoplot.clm(m1a, what = c("covariate"), x = data$Preprints)
# N.B. - Interpreting residual plots is largely subjective! 
```


#EXTRA VARIABLES

#Subject engagement
```{r}
data$group <- ifelse(data$Complete %in% c(1,2), "Low", "High")

cleaned_data <- data[!is.na(data$Subject.Engagement), ]
engagement_summary <- cleaned_data %>% group_by(group, Subject.Engagement) %>% summarise(Count = n(), .groups = 'drop') %>% mutate(Percentage = (Count / sum(Count)) * 100)
# Recreate the contingency table from the summarized data
contingency_table <- xtabs(Count ~ group + Subject.Engagement, data = engagement_summary) 
print(engagement_summary)

# Create the contingency table
contingency_table <- table(cleaned_data$group, cleaned_data$Subject.Engagement)
# If you want to print the table to see its content
print(contingency_table)

 # Perform the Chi-square test
chi_square_result <- chisq.test(contingency_table)

 # Print the result
print(chi_square_result)

```
#Remove 2 as variable - dont consider both (animal and humans) as it may taint result
```{r}
# Filter out NA and '2' values from Subject.Engagement
cleaned_data <- data[!is.na(data$Subject.Engagement) & data$Subject.Engagement != 2, ] 


# Calculate count and percentage for each group
engagement_summary <- cleaned_data %>%
  group_by(group, Subject.Engagement) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  mutate(Percentage = (Count / sum(Count)) * 100)

# Print the summary
print(engagement_summary)
# Recreate the contingency table from the summarized data
contingency_table <- xtabs(Count ~ group + Subject.Engagement, data = engagement_summary) 

print(engagement_summary)
```

```{r}
# Total count of papers involved in subject engagement before filtering
total_papers_with_engagement <- sum(!is.na(data$Subject.Engagement) & data$Subject.Engagement != 2)

# Filter out NA and '2' values from Subject.Engagement
cleaned_data <- data[!is.na(data$Subject.Engagement) & data$Subject.Engagement != 2, ] 

# Calculate count and percentage for each group
engagement_summary <- cleaned_data %>%
  group_by(group, Subject.Engagement) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  mutate(Total_Papers_With_Engagement = total_papers_with_engagement,
         Percentage = (Count/total_papers_with_engagement) *100)

# Print the summary
print(engagement_summary)
```


```{r}
# Create the contingency table
contingency_table <- table(cleaned_data$group, cleaned_data$Subject.Engagement)
# If you want to print the table to see its content
print(contingency_table)

# Perform the Chi-square test
chi_square_result <- chisq.test(contingency_table)
# Print the result
print(chi_square_result)
```

#Global vs Local disease distribution
```{r}
cleaned_data <- data[!is.na(data$Global.or.local.), ]
engagement_summary <- cleaned_data %>% 
  group_by(group, Global.or.local.) %>% 
  summarise(Count = n(), .groups = 'drop') %>% 
  mutate(Percentage = (Count / sum(Count)) * 100)

# Recreate the contingency table from the summarized data
contingency_table <- xtabs(Count ~ group + Global.or.local., data = engagement_summary) 

print(engagement_summary)

# Create the contingency table
contingency_table <- table(cleaned_data$group, cleaned_data$Global.or.local.)
# If you want to print the table to see its content
print(contingency_table)

# Perform the Chi-square test
chi_square_result <- chisq.test(contingency_table)
# Print the result
print(chi_square_result)
```


#Treatment development - Drug/Vaccine or None
```{r}
data$TreatmentLabel <- ifelse(is.na(data$Treatment.development), "No", ifelse(data$Treatment.development %in% c(0, 1), "Yes", NA))
engagement_summary <- data %>%
group_by(group, TreatmentLabel) %>% 
  summarise(Count = n(), .groups = 'drop') %>% 
  mutate(Percentage = (Count / sum(Count)) * 100)

# Recreate the contingency table from the summarized data
contingency_table <- xtabs(Count ~ group + TreatmentLabel, data = engagement_summary) 

print(engagement_summary)

# Create the contingency table
contingency_table <- table(data$group, data$TreatmentLabel)
# If you want to print the table to see its content
print(contingency_table)

# Perform the Chi-square test
chi_square_result <- chisq.test(contingency_table)
# Print the result
print(chi_square_result)

```
#Treatment development - Drug, Vaccine or NA
```{r}

treatment_summary <- data %>%
group_by(group, Treatment.development) %>% 
  summarise(Count = n(), .groups = 'drop') %>% 
  mutate(Percentage = (Count / sum(Count)) * 100)
# Recreate the contingency table from the summarized data
contingency_table <- xtabs(Count ~ group + Treatment.development, data = engagement_summary) 

print(treatment_summary)

# Create the contingency table
contingency_table <- table(data$group, data$Treatment.development)
# If you want to print the table to see its content
print(contingency_table)

# Perform the Chi-square test
chi_square_result <- chisq.test(contingency_table)
# Print the result
print(chi_square_result)
```

```{r}
# Convert NA values to a separate category
data$TreatmentLabel <- ifelse(is.na(data$Treatment.development), "NA", ifelse(data$Treatment.development %in% c(0, 1), as.character(data$Treatment.development), NA))

# Recreate the contingency table with NA as a separate category
contingency_table <- table(data$group, data$TreatmentLabel)

# If you want to print the table to see its content
print(contingency_table)

# Perform the Chi-square test
chi_square_result <- chisq.test(contingency_table)
# Print the result
print(chi_square_result)

```

#Uk vs non-uk participants
```{r}
# Recategorize completeness scores into low and high categories
data$Completeness_Category <- ifelse(data$Complete %in% c(1, 2), "Low", "High")

# Create a contingency table
table_code <- table(data$Completeness_Category, data$Participant.location)

# Perform the Chi-square test
result <- chisq.test(table_code)

# Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```

#Plot this distribution
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming your data frame is already prepared and has the variables 'Complete' and 'Participant.location'

# Recategorize completeness scores into low and high categories with detailed descriptions
data <- data %>%
  mutate(Completeness_Scores = ifelse(Complete %in% c(1, 2), "Low (1 & 2)", "High (3 & 4)"),
         Location_Label = ifelse(Participant.location == 0, "Non-UK", "UK")) %>%
  filter(!is.na(Completeness_Scores) & !is.na(Location_Label))  # Exclude rows with NA in these columns

# Create a contingency table for Chi-square test and plot input
table_code <- table(data$Completeness_Scores, data$Location_Label)

# Perform the Chi-square test
result <- chisq.test(table_code)

# Create a formatted p-value annotation string
p_value_annotation <- if(result$p.value > 0.05) {
  "n.s."
} else {
  sprintf("p = %.4f", result$p.value)
}

# Define specific colors for the categories
colors <- c("Low (1 & 2)" = "darkorange3", "High (3 & 4)" = "darkolivegreen")

# Create a bar plot
p <- ggplot(data, aes(x = Location_Label, fill = Completeness_Scores)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = colors) +
  labs(title = "Completeness Scores by Participant Location",
       x = "Location",
       y = "Count",
       fill = "Completeness Scores") +
  theme_minimal() +
  theme(legend.position = "bottom")  # Move legend to bottom

# Annotate plot with p-value or "n.s." if not significant
p <- p + geom_text(aes(x = 1.5, y = max(table_code) + 5, label = p_value_annotation), hjust = 0.5, vjust = -1)

# Print the plot
print(p)

ggsave("completeness_scores_by_location.png", plot = p, width = 10, height = 8, dpi = 300)


```




#Publishing impact metrics

#T-test for citation counts, journal IF and their mean calculation

```{r}
data$group <- ifelse(data$Complete %in% c(1,2), "Low", "High")
count <- table(data$group)

print(count)

means_values <- aggregate(cbind(Citation.Rate,Journal.IF..2022.)~ group, data= data, FUN= mean)
print(means_values)


t.test(Citation.Rate ~ group, data=data)
t.test(Journal.IF..2022. ~ group, data=data)
```

```{r}
# Load required library
library(ggplot2)

# Create scatter plot with trend line
ggplot(data, aes(x = Year, y = Citation.Rate)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +  # Add trend line
  labs(x = "Year", y = "Average Citations per Annum") +
  ggtitle("Scatter Plot of Citation Rate Over the Years")

```



```{r}
data$group <- ifelse(data$Access %in% c(1,2), "Low", "High")
count <- table(data$group)

print(count)

means_values <- aggregate(cbind(Citation.Rate,Journal.IF..2022.)~ group, data= data, FUN= mean)
print(means_values)
median_values <- aggregate(cbind(Citation.Rate,Journal.IF..2022.)~ group, data= data, FUN= median)
print(median_values)

wilcox.test(Citation.Rate ~ group, data=data)
wilcox.test(Journal.IF..2022. ~ group, data=data)

```

```{r}
datah <- data %>% 
  filter(Human.Data %in% c(0,1))
```

```{r}
table_h <- table(datah$Participant.location, datah$group)
percent_h <- prop.table(table_h, 1)*100

print(table_h)
print(percent_h)

chisq.test(table_h)
```

#Chi-square tests for participant and study conditions

#Global vs Local distribution
```{r}
# Recategorize completeness scores into low and high categories
data$Completeness_Category <- ifelse(data$Complete %in% c(1, 2), "Low", "High")

# Create a contingency table
table_code <- table(data$Completeness_Category, data$Global.or.local.)

# Perform the Chi-square test
result <- chisq.test(table_code)

# Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```
#Uk vs non-uk participants
```{r}
# Recategorize completeness scores into low and high categories
data$Completeness_Category <- ifelse(data$Complete %in% c(1, 2), "Low", "High")

# Create a contingency table
table_code <- table(data$Completeness_Category, data$Participant.location)

# Perform the Chi-square test
result <- chisq.test(table_code)

# Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```
#Treatment developed
```{r}
# Recategorize treatment development variable
data$Treatment_development <- ifelse(data$Treatment.development %in% c(0, 1), "Treatment Developed", "Treatment Not Developed")

# Recategorize completeness scores into low and high categories
data$Completeness_Category <- ifelse(data$Complete %in% c(1, 2), "Low", "High")

# Create a contingency table
table_code <- table(data$Treatment_development, data$Completeness_Category)

# Perform the Chi-square test
result <- chisq.test(table_code)

# Print the chi-square results
print(result)

# Get the expected values
expected_values <- result$expected

# Print the expected values
print(expected_values)
```




#Research Group figures (Internal)
```{r}
library(ggplot2)
library(scales)  # For percent_format

# Assuming your data frame is named 'data'
data$Complete <- factor(data$Complete, levels = c("4", "3", "2", "1"))

# Calculate the frequency and percentage for each 'Complete' score among the research Group
long_data <- data %>%
  count(ResGrp..PI.names, Complete) %>%
  group_by(ResGrp..PI.names) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ungroup() %>%
  arrange(ResGrp..PI.names, desc(Complete)) %>%
  group_by(ResGrp..PI.names) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - 0.5 * Percentage)

# Custom colors, adjust as needed
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Create the stacked bar chart
internal1 <- ggplot(long_data, aes(x = as.factor(ResGrp..PI.names), y = n, fill = as.factor(Complete))) +
  geom_bar(stat = "identity", position = "fill") +
  scale_y_continuous(labels = percent_format()) +
  geom_text(aes(label = Label, y = Cumulative_Percentage / 100),  # Ensure positioning within segments
            size = 4, 
            color = "black", 
            vjust = 0.5) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'Completeness' Scores by ResGrp",
       x = "Research Group",
       y = "Proportion",
       fill = "Complete Score") +
  theme_minimal() +
  scale_x_discrete(name = "Research Group", labels = function(x) x) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Print the plot
print(internal1)

# Save the plot
ggsave("internal1.png", internal1, width = 15, height = 10, units = "in", bg = "white")

```

```{r}
library(dplyr)
library(ggplot2)
library(scales)  # For percent_format

# Assuming your data frame is named 'data'
data$Complete <- factor(data$Complete, levels = c("4", "3", "2", "1"))

# Create a mapping of group names to letters (A, B, C, ...)
unique_groups <- unique(data$ResGrp..PI.names)
group_labels <- setNames(paste0("Group ", LETTERS[1:length(unique_groups)]), unique_groups)
data$AnonGroup <- group_labels[data$ResGrp..PI.names]

# Calculate the frequency and percentage for each 'Complete' score among the anonymous Research Group
long_data <- data %>%
  count(AnonGroup, Complete) %>%
  group_by(AnonGroup) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ungroup() %>%
  arrange(AnonGroup, desc(Complete)) %>%
  group_by(AnonGroup) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - 0.5 * Percentage)

# Custom colors, adjust as needed
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Create the stacked bar chart with anonymous labels
internal1 <- ggplot(long_data, aes(x = as.factor(AnonGroup), y = n, fill = as.factor(Complete))) +
  geom_bar(stat = "identity", position = "fill") +
  scale_y_continuous(labels = percent_format()) +
  geom_text(aes(label = Label, y = Cumulative_Percentage / 100),  # Ensure positioning within segments
            size = 4, 
            color = "black", 
            vjust = 0.5) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'Completeness' Scores by Research Group",
       x = "Research Group",
       y = "Proportion",
       fill = "Complete Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Print the plot
print(internal1)

# Save the plot
ggsave("internal1_anonymous.png", internal1, width = 15, height = 10, units = "in", bg = "white")

```



```{r}
library(ggplot2)
library(scales)  # For percent_format

# Assuming your data frame is named 'data'
data$Access <- factor(data$Access, levels = c("4", "3", "2", "1"))

# Calculate the frequency and percentage for each 'Access' score among the research Group
long_data <- data %>%
  count(ResGrp..PI.names, Access) %>%
  group_by(ResGrp..PI.names) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  ungroup()

# Recalculate Cumulative_Percentage more accurately for label placement
long_data <- long_data %>%
  arrange(ResGrp..PI.names, desc(Access)) %>%
  group_by(ResGrp..PI.names) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - 0.5 * Percentage)

# Custom colors
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Create the stacked bar chart
internal2 <- ggplot(long_data, aes(x = as.factor(ResGrp..PI.names), y = n, fill = as.factor(Access))) +
  geom_bar(stat = "identity", position = "fill") +
  scale_y_continuous(labels = percent_format()) +
  geom_text(aes(label = Label, y = Cumulative_Percentage / 100),  # Adjust label position
            size = 4, 
            color = "black", 
            vjust = 0.5) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'Accessibility' Scores by Research Group",
       x = "Research Group",
       y = "Proportion",
       fill = "Access Score") +
  theme_minimal() +
  scale_x_discrete(name = "Research Group", labels = function(x) x) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Print the plot
print(internal2)

# Save the plot
ggsave("internal2.png", internal1, width = 15, height = 10, units = "in", bg= "white")
```

#anonymous for access
```{r}
library(ggplot2)
library(scales)  # For percent_format
library(dplyr)

# Assuming your data frame is named 'data'
data$Access <- factor(data$Access, levels = c("4", "3", "2", "1"))

# Create a mapping of group names to letters (A, B, C, ...)
unique_groups <- unique(data$ResGrp..PI.names)
group_labels <- setNames(paste0("Group ", LETTERS[1:length(unique_groups)]), unique_groups)
data$AnonGroup <- group_labels[data$ResGrp..PI.names]

# Calculate the frequency and percentage for each 'Access' score among the anonymized Research Group
long_data <- data %>%
  count(AnonGroup, Access) %>%
  group_by(AnonGroup) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  ungroup()

# Recalculate Cumulative_Percentage more accurately for label placement
long_data <- long_data %>%
  arrange(AnonGroup, desc(Access)) %>%
  group_by(AnonGroup) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - 0.5 * Percentage)

# Custom colors
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Create the stacked bar chart with anonymized labels
internal2 <- ggplot(long_data, aes(x = as.factor(AnonGroup), y = n, fill = as.factor(Access))) +
  geom_bar(stat = "identity", position = "fill") +
  scale_y_continuous(labels = percent_format()) +
  geom_text(aes(label = Label, y = Cumulative_Percentage / 100),  # Adjust label position
            size = 4, 
            color = "black", 
            vjust = 0.5) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'Accessibility' Scores by Research Group",
       x = "Research Group",
       y = "Proportion",
       fill = "Access Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Print the plot
print(internal2)

# Save the plot correctly
ggsave("internal2_anonymous.png", internal2, width = 15, height = 10, units = "in", bg = "white")

```




```{r}
library(ggplot2)
library(scales)  # For percent_format

# Assuming your data frame is named 'data'
# Ensure 'Reuse' is treated as a factor and reorder its levels
data$Reuse <- factor(data$Reuse, levels = c("4", "3", "2", "1"))

# Calculate the frequency and percentage for each 'Reuse' score among the research Group
long_data <- data %>%
  count(ResGrp..PI.names, Reuse) %>%
  group_by(ResGrp..PI.names) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ungroup() %>%
  arrange(ResGrp..PI.names, desc(Reuse)) %>%
  group_by(ResGrp..PI.names) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - 0.5 * Percentage)

# Custom colors, adjust as needed
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Creating the stacked bar chart with percentage labels
internal3 <- ggplot(long_data, aes(x = as.factor(ResGrp..PI.names), y = n, fill = as.factor(Reuse))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Cumulative_Percentage / 100), 
    size = 4, 
    color = "black", 
    vjust = 0.5) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'Reusability' Scores by Research Group",
       x = "Research Group",
       y = "Proportion",
       fill = "Reuse Score") +
  theme_minimal() +
  scale_x_discrete(name = "Research Group", labels = function(x) format(x, width = 20)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(internal3)

# Save the plot
ggsave("internal3.png", internal1, width = 15, height = 10, units = "in", bg = "white")

```
#Anon Reuse
```{r}
library(ggplot2)
library(scales)  # For percent_format
library(dplyr)

# Assuming your data frame is named 'data'
data$Reuse <- factor(data$Reuse, levels = c("4", "3", "2", "1"))

# Create a mapping of group names to letters (A, B, C, ...)
unique_groups <- unique(data$ResGrp..PI.names)
group_labels <- setNames(paste0("Group ", LETTERS[1:length(unique_groups)]), unique_groups)
data$AnonGroup <- group_labels[data$ResGrp..PI.names]

# Calculate the frequency and percentage for each 'Reuse' score among the anonymized Research Group
long_data <- data %>%
  count(AnonGroup, Reuse) %>%
  group_by(AnonGroup) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ungroup() %>%
  arrange(AnonGroup, desc(Reuse)) %>%
  group_by(AnonGroup) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - 0.5 * Percentage)

# Custom colors, adjust as needed
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Creating the stacked bar chart with percentage labels using anonymized labels
internal3 <- ggplot(long_data, aes(x = as.factor(AnonGroup), y = n, fill = as.factor(Reuse))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Cumulative_Percentage / 100), 
    size = 4, 
    color = "black", 
    vjust = 0.5) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'Reusability' Scores by Research Group",
       x = "Research Group",
       y = "Proportion",
       fill = "Reuse Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(internal3)

# Correct the save function to reference the correct plot
ggsave("internal3_anonymous.png", internal3, width = 15, height = 10, units = "in", bg = "white")

```


```{r}
library(ggplot2)
library(scales)  # For percent_format

# Assuming your data frame is named 'data'
# Ensure 'Licence' is treated as a factor and reorder its levels
data$Licence <- factor(data$Licence, levels = c("4", "3", "2", "1"))

# Calculate the frequency and percentage for each 'Licence' score among the research Group
long_data <- data %>%
  count(ResGrp..PI.names, Licence) %>%
  group_by(ResGrp..PI.names) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ungroup() %>%
  arrange(ResGrp..PI.names, desc(Licence)) %>%
  group_by(ResGrp..PI.names) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - 0.5 * Percentage)

# Custom colors, adjust as needed
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Creating the stacked bar chart with percentage labels
internal4 <- ggplot(long_data, aes(x = as.factor(ResGrp..PI.names), y = n, fill = as.factor(Licence))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Cumulative_Percentage / 100), 
    size = 4, 
    color = "black", 
    vjust = 0.5) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'Licence' Scores by Research Group",
       x = "Research Group",
       y = "Proportion",
       fill = "Licence Score") +
  theme_minimal() +
  scale_x_discrete(name = "Research Group", labels = function(x) x) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(internal4)

# Save the plot
ggsave("internal4.png", internal1, width = 15, height = 10, units = "in", bg= "white")

```
#ANON LICENSE
```{r}
library(ggplot2)
library(scales)  # For percent_format
library(dplyr)

# Assuming your data frame is named 'data'
data$Licence <- factor(data$Licence, levels = c("4", "3", "2", "1"))

# Create a mapping of group names to letters (A, B, C, ...)
unique_groups <- unique(data$ResGrp..PI.names)
group_labels <- setNames(paste0("Group ", LETTERS[1:length(unique_groups)]), unique_groups)
data$AnonGroup <- group_labels[data$ResGrp..PI.names]

# Calculate the frequency and percentage for each 'Licence' score among the anonymized Research Group
long_data <- data %>%
  count(AnonGroup, Licence) %>%
  group_by(AnonGroup) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ungroup() %>%
  arrange(AnonGroup, desc(Licence)) %>%
  group_by(AnonGroup) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - 0.5 * Percentage)

# Custom colors, adjust as needed
colors <- c("#a04652", "#467fa0", "#7fa046", "#dec108")

# Creating the stacked bar chart with anonymized labels
internal4 <- ggplot(long_data, aes(x = as.factor(AnonGroup), y = n, fill = as.factor(Licence))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Cumulative_Percentage / 100), 
    size = 4, 
    color = "black", 
    vjust = 0.5) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'Licence' Scores by Research Group",
       x = "Research Group",
       y = "Proportion",
       fill = "Licence Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(internal4)

# Correct the save function to reference the correct plot
ggsave("internal4_anonymous.png", internal4, width = 15, height = 10, units = "in", bg = "white")

```

```{r}

library(ggplot2)
library(patchwork)

# Define or recreate your ggplot objects here if needed
# For demonstration, we assume internal1, internal2, internal3, and internal4 are already created

# Combine the plots using patchwork
plot_combined <- (internal1 | internal2) / 
                 (internal3 | internal4)

# Print the combined plot to check layout before saving
print(plot_combined)

# Save the combined plot
ggsave("plotinternalall.png", plot_combined, width = 20, height = 15, units = "in")
ggsave("plotinternalall_anonymous.png", plot_combined, width = 20, height = 15, units = "in")
```

```{r}
# Calculate the frequency and percentage for each 'DAS' score among the research Group
long_data <- data %>%
  count(ResGrp, NewDAS) %>%
  group_by(ResGrp) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  mutate(Label = ifelse(Percentage > 5, paste0(round(Percentage, 1), "%"), "")) %>%
  mutate(Cumulative_Percentage = cumsum(Percentage) - (0.5 * Percentage)) %>%
  ungroup()

# Custom colors
colors <- c("Not Presented" = "gray", "Not Shared" = "#E5696F", "Shared" = "#50689B")

# Creating the stacked bar chart with percentage labels
DASinternal <- ggplot(long_data, aes(x = as.factor(ResGrp), y = n, fill = as.factor(NewDAS))) +
  geom_bar(stat = "identity", position = "fill") +  # Scale the bar heights to proportions
  scale_y_continuous(labels = percent_format()) +  # Convert the y-axis to percentage
  geom_text(
    aes(label = Label, y = Percentage), 
    size = 4, 
    color = "black", 
    position = position_fill(vjust = 0.5)
  ) +
  scale_fill_manual(values = colors) +
  labs(title = "Distribution of 'DAS' Scores by ResGrp",
       x = "ResGrp",
       y = "Proportion",
       fill = "DAS") +
  theme_minimal() +
  scale_x_discrete(name = "ResGrp", labels = unique(long_data$ResGrp)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Adjust text angle for readability

# Print the plot
print(DASinternal)
ggsave("DASinternal.png", DASinternal, width = 15, height = 10, units = "in", bg= "white")

```

#Code Archived over the years
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Ensure 'data' is your dataframe and it has 'Year' and 'CodeArchived' columns
# Convert 'CodeArchived' to a factor with proper labels
data$CodeArchived <- factor(data$CodeArchived, levels = c(0, 1), labels = c("Not Shared", "Shared"))

# Filter out NA values from CodeArchived
data <- data %>% filter(!is.na(CodeArchived))

# Create a summary table
code_summary <- data %>%
  group_by(Year, CodeArchived) %>%
  summarise(Count = n(), .groups = "drop")

# Plotting the data
ggplot(code_summary, aes(x = Year, y = Count, fill = CodeArchived)) +
  geom_bar(stat = "identity", position = "dodge") +

  scale_fill_manual(values = c("red", "green")) +
  labs(title = "Yearly Distribution of Code Sharing",
       x = "Year",
       y = "Number of Papers",
       fill = "Code Sharing Status") +
  theme_minimal()

```

